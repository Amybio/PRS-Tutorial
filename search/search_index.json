{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview This tutorial provides a step-by-step guide to performing basic polygenic risk score (PRS) analyses and accompanies our PRS Guide paper . The aim of this tutorial is to provide a simple introduction of PRS analyses to those new to PRS, while equipping existing users with a better understanding of the processes and implementation \"underneath the hood\" of popular PRS software. The tutorial is separated into four main sections and reflects the structure of our guide paper : the first two sections on QC correspond to Section 2 of the paper and constitute a 'QC checklist' for PRS analyses, the third section on calculating PRS (here with examples using PLINK , PRSice-2 , LDpred and lassosum ) corresponds to Section 3 of the paper, while the fourth section, which provides some examples of visualising PRS results, accompanies Section 4 of the paper. Quality Control (QC) of Base Data Quality Control (QC) of Target Data Calculating and analysing PRS Visualising PRS Results We will be referring to our guide paper in each section and so you may find it helpful to have the paper open as you go through the tutorial. Warning Data used in this tutorial are simulated and is intended for demonstration purposes only. Result from this tutorial will not reflect the true performance of different software. Note This tutorial is written for Linux and OS X operating systems. Windows users will need to change some commands accordingly. Note Throughout the tutorial you will see tabs above some of the code: A echo Tab A B echo Tab B You can click on the tab to change to an alternative code (eg. to a different operation system) Datasets Base data : Modified summary statistics file from the GIANT consortium study on height Target data : Simulated data based on the 1000 Genomes Project European samples Requirements To follow the tutorial, you will need the following programs installed: R ( version 3.2.3+ ) PLINK 1.9 Citation If you find this tutorial helpful for a publication, then please consider citing: Citation Choi SW, Mak TSH, O'Reilly PF. A guide to performing Polygenic Risk Score analyses. bioRxiv 416545 (2018). https://doi.org/10.1101/416545","title":"Overview"},{"location":"#overview","text":"This tutorial provides a step-by-step guide to performing basic polygenic risk score (PRS) analyses and accompanies our PRS Guide paper . The aim of this tutorial is to provide a simple introduction of PRS analyses to those new to PRS, while equipping existing users with a better understanding of the processes and implementation \"underneath the hood\" of popular PRS software. The tutorial is separated into four main sections and reflects the structure of our guide paper : the first two sections on QC correspond to Section 2 of the paper and constitute a 'QC checklist' for PRS analyses, the third section on calculating PRS (here with examples using PLINK , PRSice-2 , LDpred and lassosum ) corresponds to Section 3 of the paper, while the fourth section, which provides some examples of visualising PRS results, accompanies Section 4 of the paper. Quality Control (QC) of Base Data Quality Control (QC) of Target Data Calculating and analysing PRS Visualising PRS Results We will be referring to our guide paper in each section and so you may find it helpful to have the paper open as you go through the tutorial. Warning Data used in this tutorial are simulated and is intended for demonstration purposes only. Result from this tutorial will not reflect the true performance of different software. Note This tutorial is written for Linux and OS X operating systems. Windows users will need to change some commands accordingly. Note Throughout the tutorial you will see tabs above some of the code: A echo Tab A B echo Tab B You can click on the tab to change to an alternative code (eg. to a different operation system)","title":"Overview"},{"location":"#datasets","text":"Base data : Modified summary statistics file from the GIANT consortium study on height Target data : Simulated data based on the 1000 Genomes Project European samples","title":"Datasets"},{"location":"#requirements","text":"To follow the tutorial, you will need the following programs installed: R ( version 3.2.3+ ) PLINK 1.9","title":"Requirements"},{"location":"#citation","text":"If you find this tutorial helpful for a publication, then please consider citing: Citation Choi SW, Mak TSH, O'Reilly PF. A guide to performing Polygenic Risk Score analyses. bioRxiv 416545 (2018). https://doi.org/10.1101/416545","title":"Citation"},{"location":"base/","text":"Obtaining the base data file The first step in Polygenic Risk Score (PRS) analyses is to generate or obtain the base data (GWAS summary statistics). Ideally these will correspond to the most powerful GWAS results available on the phenotype under study. In this example, we will use a modified version of the Height GWAS summary statistics generated by the GIANT consortium . You can download the summary statistic file here Note Due to limitation to bandwidth, we are currently using google drive to host the files, which doesn't allow the use of wget or curl to download the file. Please download the files manually. which will create a file called GIANT.height.gz in your working directory. Warning If you download the summary statistics without using the bash command, and are using a MAC machine, the gz file will be decompressed automatically, resulting in a GIANT.height file instead. To maintain consistency, we suggest compressing the GIANT.height file with gzip GIANT.height before starting the tutorial Reading the base data file GIANT.height.gz is compressed. To read its content, you can type: gunzip -c GIANT.height.gz | head which will display the first 10 lines of the file Note Working with compressed files reduces storage space requirements The GIANT.height.gz file contains the following columns: SNP CHR BP A1 A2 MAF SE P N INFO OR rs2073813 1 753541 A G 0.125 0.0083 0.68 69852 0.866425782879888 0.996605773454898 rs12562034 1 768448 A G 0.092 0.0088 0.55 88015 0.917520990188678 0.994714020220009 rs2980319 1 777122 A T 0.125 0.006 0.65 148975 0.847126999058955 0.997303641721713 The column headers correspond to the following: SNP : SNP ID, usually in the form of rs-ID CHR : The chromosome in which the SNP resides BP : Chromosomal co-ordinate of the SNP A1 : The effect allele of the SNP A2 : The non-effect allele of the SNP MAF : The minor allele frequency (MAF) of the SNP SE : The standard error (SE) of the effect size esimate P : The P-value of association between the SNP genotypes and the base phenotype N : Number of samples used to obtain the effect size estimate INFO : The imputation information score OR : The effect size estimate of the SNP, if the outcome is binary/case-control. If the outcome is continuous or treated as continuous then this will be the BETA QC checklist: Base data Below we perform QC on these base data according to the 'QC checklist' in our guide paper , which we recommend that users follow while going through this tutorial and when performing PRS analyses: # Heritability check We recommend that PRS analyses are performed on base data with a chip-heritability estimate \\(h_{snp}^{2} 0.05\\) . The chip-heritability of a GWAS can be estimated using e.g. LD Score Regression (LDSC). Our GIANT height GWAS data are known to have a chip-heritability much greater than 0.05 and so we can move on to the next QC step. # Effect allele The GIANT consortium report which is the effect allele and which is the non-effect allele in their results, critical for PRS association results to be in the correction direction. Important Some GWAS results files do not make clear which allele is the effect allele and which is the non-effect allele. If the incorrect assumption is made in computing the PRS, then the effect of the PRS in the target data will be in the wrong direction. To avoid misleading conclusions the effect allele from the base (GWAS) data must be known. # File transfer A common problem is that the downloaded base data file can be corrupted during download, which can cause PRS software to crash or to produce errors in results. However, a md5sum hash is generally included in files so that file integrity can be checked. The following command performs this md5sum check: Linux md5sum GIANT.height.gz OS X md5 GIANT.height.gz if the file is intact, then md5sum generates a string of characters, which in this case should be: 80e48168416a2fdbe88d68cdfebd4ca2 . If a different string is generated, then the file is corrupted. # Genome build The GIANT summary statistic are on the same genome build as the target data that we will be using. You must check that your base and target data are on the same genome build, and if they are not then use a tool such as LiftOver to make the builds consistent across the data sets. # Standard GWAS QC As described in the paper, both the base and target data should be subjected to the standard stringent QC steps performed in GWAS. If the base data have been obtained as summary statistics from a public source, then the typical QC steps that you will be able to perform on them are to filter the SNPs according to INFO score and MAF. SNPs with low minor allele frequency (MAF) or imputation information score (INFO) are more likely to generate false positive results due to their lower statistical power (and higher probability of genotyping errors in the case of low MAF). Therefore, SNPs with low MAF and INFO are typically removed before performing downstream analyses. We recommend removing SNPs with MAF 1% and INFO 0.8 (with very large base sample sizes these thresholds could be reduced if sensitivity checks indicate reliable results). These SNP filters can be achieved using the following code: gunzip -c GIANT.height.gz | \\ awk NR==1 || ($6 0.01) ($10 0.8) {print} | \\ gzip Height.gz The bash code above does the following: Decompresses and reads the GIANT.height.gz file Prints the header line ( NR==1 ) Prints any line with MAF above 0.01 ( $6 because the sixth column of the file contains the MAF information) Prints any line with INFO above 0.8 ( $10 because the tenth column of the file contains the INFO information) Compresses and writes the results to Height.gz # Ambiguous SNPs If the base and target data were generated using different genotyping chips and the chromosome strand (+/-) that was used for either is unknown, then it is not possible to pair-up the alleles of ambiguous SNPs (i.e. those with complementary alleles, either C/G or A/T SNPs) across the data sets, because it will be unknown whether the base and target data are referring to the same allele or not. While allele frequencies could be used to infer which alleles are on the same strand, the accuracy of this could be low for SNPs with MAF close to 50% or when the base and target data are from different populations. Therefore, we recommend removing all ambiguous SNPs to avoid introducing this potential source of systematic error. Ambiguous SNPs can be removed in the base data and then there will be no such SNPs in the subsequent analyses, since analyses are performed only on SNPs that overlap between the base and target data. Nonambiguous SNPs can be retained using the following: gunzip -c Height.gz | \\ awk !( ($4== A $5== T ) || \\ ($4== T $5== A ) || \\ ($4== G $5== C ) || \\ ($4== C $5== G )) {print} | \\ gzip Height.noambig.gz How many non-ambiguous SNPs were there? There are 609,041 ambiguous SNPs # Mismatching SNPs SNPs that have mismatching alleles reported in the base and target data are either resolvable by \"strand-flipping\" the alleles to their complementary alleles in e.g. the target data, such as for a SNP with A/C in the base data and G/T in the target, or non-resolvable, such as for a SNP with C/G in the base and C/T in the target. Most polygenic score software perform strand-flipping automatically for SNPs that are resolvable, and remove non-resolvable mismatching SNPs. Since we need the target data to know which SNPs have mismatching alleles, we will perform this strand-flipping in the target data. # Duplicate SNPs If an error has occurred in the generation of the base data then there may be duplicated SNPs in the base data file. Most PRS software do not allow duplicated SNPs in the base data input and thus they should be removed, using a command such as the one below: gunzip -c Height.noambig.gz | \\ awk { print $1} | \\ sort | \\ uniq -d duplicated.snp The above command does the following: Decompresses and reads the Height.noambig.gz file Prints out the first column of the file (which contains the SNP ID; change $1 to another number if the SNP ID is located in another column, e.g. $3 if the SNP ID is located on the third column) Sort the SNP IDs. This will put duplicated SNP IDs next to each other Print out any duplicated SNP IDs using the uniq command and print them to the duplicated.snp file How many duplicated SNPs are there? There are a total of 10 duplicated SNPs Duplicated SNPs can then be removed using the grep command: gunzip -c Height.noambig.gz | \\ grep -vf duplicated.snp | \\ gzip - Height.QC.gz The above command does the following: Decompresses and reads the Height.noambig.gz file From the file, remove ( -v ) any lines contains string within the duplicated.snp file ( -f ) Compresses and writes the results to Height.QC.gz # Sex chromosomes Sometimes sample mislabelling can occur, which may lead to invalid results. One indication of a mislabelled sample is a difference between reported sex and that indicated by the sex chromosomes. While this may be due to a difference in sex and gender identity, it could also reflect mislabeling of samples or misreporting and, thus, individuals in which there is a mismatch between biological and reported sex are typically removed. See the Target Data section in which a sex-check is performed. # Sample overlap Since the target data were simulated there are no overlapping samples between the base and target data here (see the relevant section of the paper for discussion of the importance of avoiding sample overlap). # Relatedness Closely related individuals within and between the base and the target data may lead to overfitted results, limiting the generalizability of the results (see the relevant sections of the paper ). Relatedness within the target data is tested in the Target Data section. The Height.QC.gz base data are now ready for using in downstream analyses.","title":"1. QC of Base Data"},{"location":"base/#obtaining-the-base-data-file","text":"The first step in Polygenic Risk Score (PRS) analyses is to generate or obtain the base data (GWAS summary statistics). Ideally these will correspond to the most powerful GWAS results available on the phenotype under study. In this example, we will use a modified version of the Height GWAS summary statistics generated by the GIANT consortium . You can download the summary statistic file here Note Due to limitation to bandwidth, we are currently using google drive to host the files, which doesn't allow the use of wget or curl to download the file. Please download the files manually. which will create a file called GIANT.height.gz in your working directory. Warning If you download the summary statistics without using the bash command, and are using a MAC machine, the gz file will be decompressed automatically, resulting in a GIANT.height file instead. To maintain consistency, we suggest compressing the GIANT.height file with gzip GIANT.height before starting the tutorial","title":"Obtaining the base data file"},{"location":"base/#reading-the-base-data-file","text":"GIANT.height.gz is compressed. To read its content, you can type: gunzip -c GIANT.height.gz | head which will display the first 10 lines of the file Note Working with compressed files reduces storage space requirements The GIANT.height.gz file contains the following columns: SNP CHR BP A1 A2 MAF SE P N INFO OR rs2073813 1 753541 A G 0.125 0.0083 0.68 69852 0.866425782879888 0.996605773454898 rs12562034 1 768448 A G 0.092 0.0088 0.55 88015 0.917520990188678 0.994714020220009 rs2980319 1 777122 A T 0.125 0.006 0.65 148975 0.847126999058955 0.997303641721713 The column headers correspond to the following: SNP : SNP ID, usually in the form of rs-ID CHR : The chromosome in which the SNP resides BP : Chromosomal co-ordinate of the SNP A1 : The effect allele of the SNP A2 : The non-effect allele of the SNP MAF : The minor allele frequency (MAF) of the SNP SE : The standard error (SE) of the effect size esimate P : The P-value of association between the SNP genotypes and the base phenotype N : Number of samples used to obtain the effect size estimate INFO : The imputation information score OR : The effect size estimate of the SNP, if the outcome is binary/case-control. If the outcome is continuous or treated as continuous then this will be the BETA","title":"Reading the base data file"},{"location":"base/#qc-checklist-base-data","text":"Below we perform QC on these base data according to the 'QC checklist' in our guide paper , which we recommend that users follow while going through this tutorial and when performing PRS analyses:","title":"QC checklist: Base data"},{"location":"base/#35-heritability-check","text":"We recommend that PRS analyses are performed on base data with a chip-heritability estimate \\(h_{snp}^{2} 0.05\\) . The chip-heritability of a GWAS can be estimated using e.g. LD Score Regression (LDSC). Our GIANT height GWAS data are known to have a chip-heritability much greater than 0.05 and so we can move on to the next QC step.","title":"# Heritability check"},{"location":"base/#35-effect-allele","text":"The GIANT consortium report which is the effect allele and which is the non-effect allele in their results, critical for PRS association results to be in the correction direction. Important Some GWAS results files do not make clear which allele is the effect allele and which is the non-effect allele. If the incorrect assumption is made in computing the PRS, then the effect of the PRS in the target data will be in the wrong direction. To avoid misleading conclusions the effect allele from the base (GWAS) data must be known.","title":"# Effect allele"},{"location":"base/#35-file-transfer","text":"A common problem is that the downloaded base data file can be corrupted during download, which can cause PRS software to crash or to produce errors in results. However, a md5sum hash is generally included in files so that file integrity can be checked. The following command performs this md5sum check: Linux md5sum GIANT.height.gz OS X md5 GIANT.height.gz if the file is intact, then md5sum generates a string of characters, which in this case should be: 80e48168416a2fdbe88d68cdfebd4ca2 . If a different string is generated, then the file is corrupted.","title":"# File transfer"},{"location":"base/#35-genome-build","text":"The GIANT summary statistic are on the same genome build as the target data that we will be using. You must check that your base and target data are on the same genome build, and if they are not then use a tool such as LiftOver to make the builds consistent across the data sets.","title":"# Genome build"},{"location":"base/#35-standard-gwas-qc","text":"As described in the paper, both the base and target data should be subjected to the standard stringent QC steps performed in GWAS. If the base data have been obtained as summary statistics from a public source, then the typical QC steps that you will be able to perform on them are to filter the SNPs according to INFO score and MAF. SNPs with low minor allele frequency (MAF) or imputation information score (INFO) are more likely to generate false positive results due to their lower statistical power (and higher probability of genotyping errors in the case of low MAF). Therefore, SNPs with low MAF and INFO are typically removed before performing downstream analyses. We recommend removing SNPs with MAF 1% and INFO 0.8 (with very large base sample sizes these thresholds could be reduced if sensitivity checks indicate reliable results). These SNP filters can be achieved using the following code: gunzip -c GIANT.height.gz | \\ awk NR==1 || ($6 0.01) ($10 0.8) {print} | \\ gzip Height.gz The bash code above does the following: Decompresses and reads the GIANT.height.gz file Prints the header line ( NR==1 ) Prints any line with MAF above 0.01 ( $6 because the sixth column of the file contains the MAF information) Prints any line with INFO above 0.8 ( $10 because the tenth column of the file contains the INFO information) Compresses and writes the results to Height.gz","title":"# Standard GWAS QC"},{"location":"base/#35-ambiguous-snps","text":"If the base and target data were generated using different genotyping chips and the chromosome strand (+/-) that was used for either is unknown, then it is not possible to pair-up the alleles of ambiguous SNPs (i.e. those with complementary alleles, either C/G or A/T SNPs) across the data sets, because it will be unknown whether the base and target data are referring to the same allele or not. While allele frequencies could be used to infer which alleles are on the same strand, the accuracy of this could be low for SNPs with MAF close to 50% or when the base and target data are from different populations. Therefore, we recommend removing all ambiguous SNPs to avoid introducing this potential source of systematic error. Ambiguous SNPs can be removed in the base data and then there will be no such SNPs in the subsequent analyses, since analyses are performed only on SNPs that overlap between the base and target data. Nonambiguous SNPs can be retained using the following: gunzip -c Height.gz | \\ awk !( ($4== A $5== T ) || \\ ($4== T $5== A ) || \\ ($4== G $5== C ) || \\ ($4== C $5== G )) {print} | \\ gzip Height.noambig.gz How many non-ambiguous SNPs were there? There are 609,041 ambiguous SNPs","title":"# Ambiguous SNPs"},{"location":"base/#35-mismatching-snps","text":"SNPs that have mismatching alleles reported in the base and target data are either resolvable by \"strand-flipping\" the alleles to their complementary alleles in e.g. the target data, such as for a SNP with A/C in the base data and G/T in the target, or non-resolvable, such as for a SNP with C/G in the base and C/T in the target. Most polygenic score software perform strand-flipping automatically for SNPs that are resolvable, and remove non-resolvable mismatching SNPs. Since we need the target data to know which SNPs have mismatching alleles, we will perform this strand-flipping in the target data.","title":"# Mismatching SNPs"},{"location":"base/#35-duplicate-snps","text":"If an error has occurred in the generation of the base data then there may be duplicated SNPs in the base data file. Most PRS software do not allow duplicated SNPs in the base data input and thus they should be removed, using a command such as the one below: gunzip -c Height.noambig.gz | \\ awk { print $1} | \\ sort | \\ uniq -d duplicated.snp The above command does the following: Decompresses and reads the Height.noambig.gz file Prints out the first column of the file (which contains the SNP ID; change $1 to another number if the SNP ID is located in another column, e.g. $3 if the SNP ID is located on the third column) Sort the SNP IDs. This will put duplicated SNP IDs next to each other Print out any duplicated SNP IDs using the uniq command and print them to the duplicated.snp file How many duplicated SNPs are there? There are a total of 10 duplicated SNPs Duplicated SNPs can then be removed using the grep command: gunzip -c Height.noambig.gz | \\ grep -vf duplicated.snp | \\ gzip - Height.QC.gz The above command does the following: Decompresses and reads the Height.noambig.gz file From the file, remove ( -v ) any lines contains string within the duplicated.snp file ( -f ) Compresses and writes the results to Height.QC.gz","title":"# Duplicate SNPs"},{"location":"base/#35-sex-chromosomes","text":"Sometimes sample mislabelling can occur, which may lead to invalid results. One indication of a mislabelled sample is a difference between reported sex and that indicated by the sex chromosomes. While this may be due to a difference in sex and gender identity, it could also reflect mislabeling of samples or misreporting and, thus, individuals in which there is a mismatch between biological and reported sex are typically removed. See the Target Data section in which a sex-check is performed.","title":"# Sex chromosomes"},{"location":"base/#35-sample-overlap","text":"Since the target data were simulated there are no overlapping samples between the base and target data here (see the relevant section of the paper for discussion of the importance of avoiding sample overlap).","title":"# Sample overlap"},{"location":"base/#35-relatedness","text":"Closely related individuals within and between the base and the target data may lead to overfitted results, limiting the generalizability of the results (see the relevant sections of the paper ). Relatedness within the target data is tested in the Target Data section. The Height.QC.gz base data are now ready for using in downstream analyses.","title":"# Relatedness"},{"location":"lassosum/","text":"Here we use another PRS program, lassosum , which is an R package that uses penalised regression (LASSO) in its approach to PRS calculation. Note The script used here is based on lassosum version 0.4.4 Note For more details, please refer to lassosum's homepage You can install lassosum and its dependencies in R with the following command: install.packages ( c ( devtools , RcppArmadillo , data.table , Matrix ), dependencies = TRUE ) library ( devtools ) install_github ( tshmak/lassosum ) Again, we assume that we have the following files (or you can download it from here ): File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.height This file contains the phenotype of the samples EUR.covariate This file contains the covariates of the samples EUR.eigenvec This file contains the PCs of the samples Running PRS analysis We can run lassosum as follows: library ( lassosum ) # Prefer to work with data.table as it speeds up file reading library ( data.table ) library ( methods ) library ( magrittr ) sum.stat - Height.QC.gz bfile - EUR.QC # Read in and process the covariates covariate - fread ( EUR.covariate ) pcs - fread ( EUR.eigenvec ) % % setnames ( . , colnames ( . ), c ( FID , IID , paste0 ( PC , 1 : 6 ))) # Need as.data.frame here as lassosum doesn t handle data.table # covariates very well cov - merge ( covariate , pcs ) # We will need the EUR.hg19 file provided by lassosum # which are LD regions defined in Berisa and Pickrell (2015) for the European population and the hg19 genome. ld.file - system.file ( data , Berisa.EUR.hg19.bed , package = lassosum ) # output prefix prefix - EUR # Read in the target phenotype file target.pheno - fread ( EUR.height )[, c ( FID , IID , Height )] # Read in the summary statistics ss - fread ( sum.stat ) # Number of sample in base size - 253288 # Remove P-value = 0, which causes problem in the transformation ss - ss [ ! P == 0 ] # Read in the LD blocks ld - fread ( ld.file ) # Transform the P-values into correlation cor - p2cor ( p = ss $ P , n = size , sign = log ( ss $ OR ) ) fam - fread ( paste0 ( bfile , .fam )) fam [, ID := do.call ( paste , c ( . SD , sep = : )), . SDcols = c ( 1 : 2 )] # Run the lassosum pipeline out - lassosum.pipeline ( cor = cor , chr = ss $ CHR , pos = ss $ BP , A1 = ss $ A1 , A2 = ss $ A2 , ref.bfile = bfile , test.bfile = bfile , LDblocks = ld ) # Store the R2 results target.res - validate ( out , pheno = target.pheno , covar = cov ) # Get the maximum R2 r2 - max ( target.res $ validation.table $ value ) ^ 2 How much phenotypic variation does the \"best-fit\" PRS explain? 0.03818004","title":"lassosum"},{"location":"lassosum/#running-prs-analysis","text":"We can run lassosum as follows: library ( lassosum ) # Prefer to work with data.table as it speeds up file reading library ( data.table ) library ( methods ) library ( magrittr ) sum.stat - Height.QC.gz bfile - EUR.QC # Read in and process the covariates covariate - fread ( EUR.covariate ) pcs - fread ( EUR.eigenvec ) % % setnames ( . , colnames ( . ), c ( FID , IID , paste0 ( PC , 1 : 6 ))) # Need as.data.frame here as lassosum doesn t handle data.table # covariates very well cov - merge ( covariate , pcs ) # We will need the EUR.hg19 file provided by lassosum # which are LD regions defined in Berisa and Pickrell (2015) for the European population and the hg19 genome. ld.file - system.file ( data , Berisa.EUR.hg19.bed , package = lassosum ) # output prefix prefix - EUR # Read in the target phenotype file target.pheno - fread ( EUR.height )[, c ( FID , IID , Height )] # Read in the summary statistics ss - fread ( sum.stat ) # Number of sample in base size - 253288 # Remove P-value = 0, which causes problem in the transformation ss - ss [ ! P == 0 ] # Read in the LD blocks ld - fread ( ld.file ) # Transform the P-values into correlation cor - p2cor ( p = ss $ P , n = size , sign = log ( ss $ OR ) ) fam - fread ( paste0 ( bfile , .fam )) fam [, ID := do.call ( paste , c ( . SD , sep = : )), . SDcols = c ( 1 : 2 )] # Run the lassosum pipeline out - lassosum.pipeline ( cor = cor , chr = ss $ CHR , pos = ss $ BP , A1 = ss $ A1 , A2 = ss $ A2 , ref.bfile = bfile , test.bfile = bfile , LDblocks = ld ) # Store the R2 results target.res - validate ( out , pheno = target.pheno , covar = cov ) # Get the maximum R2 r2 - max ( target.res $ validation.table $ value ) ^ 2 How much phenotypic variation does the \"best-fit\" PRS explain? 0.03818004","title":"Running PRS analysis"},{"location":"ldpred/","text":"Here we use another PRS program, LDpred , that uses a Bayesian approach to polygenic risk scoring. Note The script used here is based on LDpred 2 implemented under bigsnpr version 1.4.7 Note For more details, please refer to LDpred 2's homepage You can install LDpred and its dependencies in R with the following command: install.packages ( remotes ) library ( remotes ) remotes :: install_github ( https://github.com/privefl/bigsnpr.git ) Note For mac users, you might need to follow the guide here to be able to install LDpred2 We assume that you have the following files (or you can download it from here ): File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.height This file contains the phenotype of the samples EUR.covariate This file contains the covariates of the samples EUR.eigenvec This file contains the PCs of the samples Running PRS analysis Read in the phenotype and covariate files library ( data.table ) library ( magrittr ) phenotype - fread ( EUR.height ) covariate - fread ( EUR.covariate ) pcs - fread ( EUR.eigenvec ) # rename columns colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) # generate required table pheno - merge ( phenotype , covariate ) % % merge ( . , pcs ) Load in the genotype and summary statistic file using bigsnpr # load the library library ( bigsnpr ) # now preprocess the bed file. Only need to do once for each data set snp_readBed ( EUR.QC.bed ) # now attach the genotype object obj.bigSNP - snp_attach ( EUR.QC.rds ) # Next, read in the summary statistic file sumstats - bigreadr :: fread2 ( Height.QC.gz ) Reformat data in preparation of LDpred2 # LDpred 2 require the header to follow the exact naming names ( sumstats ) - c ( rsid , chr , pos , a1 , a0 , MAF , beta_se , p , n_eff , INFO , OR ) # Transform the OR into log(OR) # Our SE is the SE of beta. # If the SE is the SE of OR, you will need to transform it (which we will not go into) sumstats $ beta - log ( sumstats $ OR ) # extract the SNP information from the genotype map - obj.bigSNP $ map [ - ( 2 : 3 )] names ( map ) - c ( chr , pos , a1 , a0 ) # perform SNP matching info_snp - snp_match ( sumstats , map ) # Rename the data structures CHR - obj.bigSNP $ map $ chromosome POS - obj.bigSNP $ map $ physical.pos # get the CM information from 1000 Genome, will download the 1000G file POS2 - snp_asGeneticPos ( CHR , POS , dir = . ) # Reformat the phenotype information to match the fam order fam.order - as.data.table ( obj.bigSNP $ fam ) setnames ( fam.order , c ( family.ID , sample.ID ), c ( FID , IID )) # This will ensure our phenotype vector y is of the same order as the # family object generated by bigsnpr y - pheno [ fam.order , on = c ( FID , IID )] genotype - obj.bigSNP $ genotypes NCORES - nb_cores () # Use all sample for test ind.test - 1 : nrow ( genotype ) # Prepare data for grid model p_seq - signif ( seq_log ( 1e-4 , 1 , length.out = 17 ), 2 ) # Calculate the null R2 # use glm for binary trait (will also need the fmsb package to calculate the pseudo R2) null.model - paste ( PC , 1 : 6 , sep = , collapse = + ) % % paste0 ( Height~Sex+ , . ) % % as.formula % % lm ( . , data = y ) % % summary null.r2 - null.model $ r.squared reg.formula - paste ( PC , 1 : 6 , sep = , collapse = + ) % % paste0 ( Height~PRS+Sex+ , . ) % % as.formula Start running LDpred 2 # Start doing analysis on each chromosome y [, Inf.est := 0 ] y [, auto := 0 ] y [, grid := 0 ] # add progress bar pb = txtProgressBar ( min = 0 , max = 22 , initial = 0 ) for ( chr in 1 : 22 ){ setTxtProgressBar ( pb , chr ) chr.idx - which ( info_snp $ chr == chr ) df_beta - info_snp [ chr.idx , c ( beta , beta_se , n_eff )] ind.chr - info_snp $ `_NUM_ID_` [ chr.idx ] corr0 - snp_cor ( genotype , ind.col = ind.chr , ncores = NCORES , infos.pos = POS2 [ ind.chr ], size = 3 / 1000 ) corr - bigsparser :: as_SFBM ( as ( corr0 , dgCMatrix )) ldsc - snp_ldsc2 ( corr0 , df_beta ) h2_est - ldsc [[ h2 ]] h2_seq - round ( h2_est * c ( 0.7 , 1 , 1.4 ), 4 ) grid.param - expand.grid ( p = p_seq , h2 = h2_seq , sparse = c ( FALSE , TRUE )) # Get adjusted beta from infinitesimal model beta_inf - snp_ldpred2_inf ( corr , df_beta , h2 = h2_est ) # Get adjusted beta from grid model beta_grid - snp_ldpred2_grid ( corr , df_beta , grid.param , ncores = NCORES ) # Get adjusted beta from the auto model multi_auto - snp_ldpred2_auto ( corr , df_beta , h2_init = h2_est , vec_p_init = seq_log ( 1e-4 , 0.9 , length.out = NCORES ), ncores = NCORES ) pred_auto - big_prodMat ( genotype , beta_auto , ind.row = ind.val , ind.col = ind.chr2 ) # scale the PRS generated from AUTO pred_scaled - apply ( pred_auto , 2 , sd ) final_beta_auto - rowMeans ( beta_auto [, abs ( sc - median ( sc )) 3 * mad ( sc )]) pred_auto - big_prodMat ( genotype , final_beta_auto , ind.row = ind.val , ind.col = ind.chr2 ) # Get infinitesimal PRS pred_inf - big_prodVec ( genotype , beta_inf , ind.row = ind.test , ind.col = ind.chr ) # Get the grid PRSs pred_grid - big_prodMat ( genotype , beta_grid , ind.col = ind.chr ) # Get the Auto PRS pred_auto - big_prodMat ( genotype , beta_auto , ind.row = ind.test , ind.col = ind.chr ) # add up the calculated PRS y [, Inf.est := Inf.est + pred_inf ] # for auto and grid, we want to retain the best prs only? indep - copy ( y ) max.grid.r2 - 0 max.grid.id - 0 for ( i in 1 : ncol ( pred_grid )){ indep [, PRS := pred_grid [, i ]] model - lm ( reg.formula , data = indep ) % % summary prs.r2 - model $ r.squared - null.r2 if ( max.grid.r2 prs.r2 ){ max.grid.r2 - prs.r2 max.grid.id - i } } # add up the calculated grid PRS y [, grid := grid + pred_grid [, max.grid.id ]] # Do the same for auto max.auto.r2 - 0 max.auto.id - 0 for ( i in 1 : ncol ( pred_auto )){ indep [, PRS := pred_auto [, i ]] model - lm ( reg.formula , data = indep ) % % summary prs.r2 - model $ r.squared - null.r2 if ( max.auto.r2 prs.r2 ){ max.auto.r2 - prs.r2 max.auto.id - i } } # add up the calculated grid PRS y [, auto := auto + pred_auto [, max.auto.id ]] } print ( Completed ) Get the final performance of the LDpred models # there are 3 models: inf, auto and grid inf.model - paste ( PC , 1 : 6 , sep = , collapse = + ) % % paste0 ( Height~Inf.est+Sex+ , . ) % % as.formula % % lm ( . , data = y ) % % summary auto.model - paste ( PC , 1 : 6 , sep = , collapse = + ) % % paste0 ( Height~auto+Sex+ , . ) % % as.formula % % lm ( . , data = y ) % % summary grid.model - paste ( PC , 1 : 6 , sep = , collapse = + ) % % paste0 ( Height~grid+Sex+ , . ) % % as.formula % % lm ( . , data = y ) % % summary result - data.table ( infinitesimal = inf.model $ r.squared - null.r2 , grid = grid.model $ r.squared - null.r2 , auto = auto.model $ r.squared - null.r2 , null = null.r2 ) print ( result )","title":"LDpred"},{"location":"ldpred/#running-prs-analysis","text":"Read in the phenotype and covariate files library ( data.table ) library ( magrittr ) phenotype - fread ( EUR.height ) covariate - fread ( EUR.covariate ) pcs - fread ( EUR.eigenvec ) # rename columns colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) # generate required table pheno - merge ( phenotype , covariate ) % % merge ( . , pcs ) Load in the genotype and summary statistic file using bigsnpr # load the library library ( bigsnpr ) # now preprocess the bed file. Only need to do once for each data set snp_readBed ( EUR.QC.bed ) # now attach the genotype object obj.bigSNP - snp_attach ( EUR.QC.rds ) # Next, read in the summary statistic file sumstats - bigreadr :: fread2 ( Height.QC.gz ) Reformat data in preparation of LDpred2 # LDpred 2 require the header to follow the exact naming names ( sumstats ) - c ( rsid , chr , pos , a1 , a0 , MAF , beta_se , p , n_eff , INFO , OR ) # Transform the OR into log(OR) # Our SE is the SE of beta. # If the SE is the SE of OR, you will need to transform it (which we will not go into) sumstats $ beta - log ( sumstats $ OR ) # extract the SNP information from the genotype map - obj.bigSNP $ map [ - ( 2 : 3 )] names ( map ) - c ( chr , pos , a1 , a0 ) # perform SNP matching info_snp - snp_match ( sumstats , map ) # Rename the data structures CHR - obj.bigSNP $ map $ chromosome POS - obj.bigSNP $ map $ physical.pos # get the CM information from 1000 Genome, will download the 1000G file POS2 - snp_asGeneticPos ( CHR , POS , dir = . ) # Reformat the phenotype information to match the fam order fam.order - as.data.table ( obj.bigSNP $ fam ) setnames ( fam.order , c ( family.ID , sample.ID ), c ( FID , IID )) # This will ensure our phenotype vector y is of the same order as the # family object generated by bigsnpr y - pheno [ fam.order , on = c ( FID , IID )] genotype - obj.bigSNP $ genotypes NCORES - nb_cores () # Use all sample for test ind.test - 1 : nrow ( genotype ) # Prepare data for grid model p_seq - signif ( seq_log ( 1e-4 , 1 , length.out = 17 ), 2 ) # Calculate the null R2 # use glm for binary trait (will also need the fmsb package to calculate the pseudo R2) null.model - paste ( PC , 1 : 6 , sep = , collapse = + ) % % paste0 ( Height~Sex+ , . ) % % as.formula % % lm ( . , data = y ) % % summary null.r2 - null.model $ r.squared reg.formula - paste ( PC , 1 : 6 , sep = , collapse = + ) % % paste0 ( Height~PRS+Sex+ , . ) % % as.formula Start running LDpred 2 # Start doing analysis on each chromosome y [, Inf.est := 0 ] y [, auto := 0 ] y [, grid := 0 ] # add progress bar pb = txtProgressBar ( min = 0 , max = 22 , initial = 0 ) for ( chr in 1 : 22 ){ setTxtProgressBar ( pb , chr ) chr.idx - which ( info_snp $ chr == chr ) df_beta - info_snp [ chr.idx , c ( beta , beta_se , n_eff )] ind.chr - info_snp $ `_NUM_ID_` [ chr.idx ] corr0 - snp_cor ( genotype , ind.col = ind.chr , ncores = NCORES , infos.pos = POS2 [ ind.chr ], size = 3 / 1000 ) corr - bigsparser :: as_SFBM ( as ( corr0 , dgCMatrix )) ldsc - snp_ldsc2 ( corr0 , df_beta ) h2_est - ldsc [[ h2 ]] h2_seq - round ( h2_est * c ( 0.7 , 1 , 1.4 ), 4 ) grid.param - expand.grid ( p = p_seq , h2 = h2_seq , sparse = c ( FALSE , TRUE )) # Get adjusted beta from infinitesimal model beta_inf - snp_ldpred2_inf ( corr , df_beta , h2 = h2_est ) # Get adjusted beta from grid model beta_grid - snp_ldpred2_grid ( corr , df_beta , grid.param , ncores = NCORES ) # Get adjusted beta from the auto model multi_auto - snp_ldpred2_auto ( corr , df_beta , h2_init = h2_est , vec_p_init = seq_log ( 1e-4 , 0.9 , length.out = NCORES ), ncores = NCORES ) pred_auto - big_prodMat ( genotype , beta_auto , ind.row = ind.val , ind.col = ind.chr2 ) # scale the PRS generated from AUTO pred_scaled - apply ( pred_auto , 2 , sd ) final_beta_auto - rowMeans ( beta_auto [, abs ( sc - median ( sc )) 3 * mad ( sc )]) pred_auto - big_prodMat ( genotype , final_beta_auto , ind.row = ind.val , ind.col = ind.chr2 ) # Get infinitesimal PRS pred_inf - big_prodVec ( genotype , beta_inf , ind.row = ind.test , ind.col = ind.chr ) # Get the grid PRSs pred_grid - big_prodMat ( genotype , beta_grid , ind.col = ind.chr ) # Get the Auto PRS pred_auto - big_prodMat ( genotype , beta_auto , ind.row = ind.test , ind.col = ind.chr ) # add up the calculated PRS y [, Inf.est := Inf.est + pred_inf ] # for auto and grid, we want to retain the best prs only? indep - copy ( y ) max.grid.r2 - 0 max.grid.id - 0 for ( i in 1 : ncol ( pred_grid )){ indep [, PRS := pred_grid [, i ]] model - lm ( reg.formula , data = indep ) % % summary prs.r2 - model $ r.squared - null.r2 if ( max.grid.r2 prs.r2 ){ max.grid.r2 - prs.r2 max.grid.id - i } } # add up the calculated grid PRS y [, grid := grid + pred_grid [, max.grid.id ]] # Do the same for auto max.auto.r2 - 0 max.auto.id - 0 for ( i in 1 : ncol ( pred_auto )){ indep [, PRS := pred_auto [, i ]] model - lm ( reg.formula , data = indep ) % % summary prs.r2 - model $ r.squared - null.r2 if ( max.auto.r2 prs.r2 ){ max.auto.r2 - prs.r2 max.auto.id - i } } # add up the calculated grid PRS y [, auto := auto + pred_auto [, max.auto.id ]] } print ( Completed ) Get the final performance of the LDpred models # there are 3 models: inf, auto and grid inf.model - paste ( PC , 1 : 6 , sep = , collapse = + ) % % paste0 ( Height~Inf.est+Sex+ , . ) % % as.formula % % lm ( . , data = y ) % % summary auto.model - paste ( PC , 1 : 6 , sep = , collapse = + ) % % paste0 ( Height~auto+Sex+ , . ) % % as.formula % % lm ( . , data = y ) % % summary grid.model - paste ( PC , 1 : 6 , sep = , collapse = + ) % % paste0 ( Height~grid+Sex+ , . ) % % as.formula % % lm ( . , data = y ) % % summary result - data.table ( infinitesimal = inf.model $ r.squared - null.r2 , grid = grid.model $ r.squared - null.r2 , auto = auto.model $ r.squared - null.r2 , null = null.r2 ) print ( result )","title":"Running PRS analysis"},{"location":"plink/","text":"Background In this section of the tutorial you will use four different software programs to compute PRS from the base and target data that you QC'ed in the previous two sections. On this page, you will compute PRS using the popular genetic analyses tool plink - while plink is not a dedicated PRS software, you can perform every required steps of the C+T approach with plink . This multi-step process is a good way to learn the processes involved in computing PRS, which are typically performed automatically by PRS software. Required Data In the previous sections, we have generated the following files: File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.height This file contains the phenotype of the samples EUR.covariate This file contains the covariates of the samples Update Effect Size When the effect size relates to disease risk and is thus given as an odds ratio (OR), rather than BETA (for continuous traits), then the PRS is computed as a product of ORs. To simplify this calculation, we take the natural logarithm of the OR so that the PRS can be computed using summation instead (which can be back-transformed afterwards). We can obtain the transformed summary statistics with R : Without data.table dat - read.table ( gzfile ( Height.QC.gz ), header = T ) dat $ OR - log ( dat $ OR ) write.table ( dat , Height.QC.Transformed , quote = F , row.names = F ) q () # exit R With data.table library ( data.table ) dat - fread ( Height.QC.gz ) fwrite ( dat [, OR := log ( OR )], Height.QC.Transformed , sep = \\t ) q () # exit R Warning Due to rounding of values, using awk to log transform OR can lead to less accurate results. Therefore, we recommend performing the transformation in R or allow the PRS software to perform the transformation directly. Clumping Linkage disequilibrium, which corresponds to the correlation between the genotypes of genetic variants across the genome, makes identifying the contribution from causal independent genetic variants extremely challenging. One way of approximately capturing the right level of causal signal is to perform clumping, which removes SNPs in ways that only weakly correlated SNPs are retained but preferentially retaining the SNPs most associated with the phenotype under study. Clumping can be performed using the following command in plink : plink \\ --bfile EUR.QC \\ --clump-p1 1 \\ --clump-r2 0 .1 \\ --clump-kb 250 \\ --clump Height.QC.Transformed \\ --clump-snp-field SNP \\ --clump-field P \\ --out EUR Each of the new parameters corresponds to the following Paramter Value Description clump-p1 1 P-value threshold for a SNP to be included as an index SNP. 1 is selected such that all SNPs are include for clumping clump-r2 0.1 SNPs having \\(r^2\\) higher than 0.1 with the index SNPs will be removed clump-kb 250 SNPs within 250k of the index SNP are considered for clumping clump Height.QC.Transformed Base data (summary statistic) file containing the P-value information clump-snp-field SNP Specifies that the column SNP contains the SNP IDs clump-field P Specifies that the column P contains the P-value information A more detailed description of the clumping process can be found here Note The \\(r^2\\) values computed by --clump are based on maximum likelihood haplotype frequency estimates This will generate EUR.clumped , containing the index SNPs after clumping is performed. We can extract the index SNP ID by performing the following command: awk NR!=1{print $3} EUR.clumped EUR.valid.snp $3 because the third column contains the SNP ID Note If your target data are small (e.g. N 500) then you can use the 1000 Genomes Project samples for the LD calculation. Make sure to use the population that most closely reflects represents the base sample. Generate PRS plink provides a convenient function --score and --q-score-range for calculating polygenic scores. We will need three files: The base data file: Height.QC.Transformed A file containing SNP IDs and their corresponding P-values ( $1 because SNP ID is located in the first column; $8 because the P-value is located in the eighth column) awk {print $1,$8} Height.QC.Transformed SNP.pvalue A file containing the different P-value thresholds for inclusion of SNPs in the PRS. Here calculate PRS corresponding to a few thresholds for illustration purposes: echo 0.001 0 0.001 range_list echo 0.05 0 0.05 range_list echo 0.1 0 0.1 range_list echo 0.2 0 0.2 range_list echo 0.3 0 0.3 range_list echo 0.4 0 0.4 range_list echo 0.5 0 0.5 range_list The format of the range_list file should be as follows: Name of Threshold Lower bound Upper Bound Note The threshold boundaries are inclusive. For example, for the 0.05 threshold, we include all SNPs with P-value from 0 to 0.05 , including any SNPs with P-value equal to 0.05 . We can then calculate the PRS with the following plink command: plink \\ --bfile EUR.QC \\ --score Height.QC.Transformed 1 4 11 header \\ --q-score-range range_list SNP.pvalue \\ --extract EUR.valid.snp \\ --out EUR The meaning of the new parameters are as follows: Paramter Value Description score Height.QC.Transformed 1 4 11 header We read from the Height.QC.Transformed file, assuming that the 1 st column is the SNP ID; 4 th column is the effective allele information; the 11 th column is the effect size estimate; and that the file contains a header q-score-range range_test SNP.pvalue We want to calculate PRS based on the thresholds defined in range_test , where the threshold values (P-values) were stored in SNP.pvalue The above command and range_list will generate 7 files: EUR.0.5.profile EUR.0.4.profile EUR.0.3.profile EUR.0.2.profile EUR.0.1.profile EUR.0.05.profile EUR.0.001.profile Note The default formula for PRS calculation in PLINK is: \\[ PRS_j =\\frac{ \\sum_i^NS_i*G_{ij}}{P*M_j} \\] where the effect size of SNP \\(i\\) is \\(S_i\\) ; the number of effect alleles observed in sample \\(j\\) is \\(G_{ij}\\) ; the ploidy of the sample is \\(P\\) (is generally 2 for humans); the total number of SNPs included in the PRS is \\(N\\) ; and the number of non-missing SNPs observed in sample \\(j\\) is \\(M_j\\) . If the sample has a missing genotype for SNP \\(i\\) , then the population minor allele frequency multiplied by the ploidy ( \\(MAF_i*P\\) ) is used instead of \\(G_{ij}\\) . Accounting for Population Stratification Population structure is the principal source of confounding in GWAS and is usually accounted for by incorporating principal components (PCs) as covariates. We can incorporate PCs into our PRS analysis to account for population stratification. Again, we can calculate the PCs using plink : # First, we need to perform prunning plink \\ --bfile EUR.QC \\ --indep-pairwise 200 50 0 .25 \\ --out EUR # Then we calculate the first 6 PCs plink \\ --bfile EUR.QC \\ --extract EUR.prune.in \\ --pca 6 \\ --out EUR Note One way to select the appropriate number of PCs is to perform GWAS on the phenotype under study with different numbers of PCs. LDSC analysis can then be performed on the set of GWAS summary statistics and the GWAS that used the number of PCs that gave an LDSC intercept closest to 1 should correspond to that for which population structure was most accurately controlled for. Here the PCs have been stored in the EUR.eigenvec file and can be used as covariates in the regression model to account for population stratification. Important If the base and target samples are collected from different worldwide populations then the results from the PRS analysis may be biased (see Section 3.4 of our papper). Finding the \"best-fit\" PRS The P-value threshold that provides the \"best-fit\" PRS under the C+T method is usually unknown. To approximate the \"best-fit\" PRS, we can perform a regression between PRS calculated at a range of P-value thresholds and then select the PRS that explains the highest phenotypic variance (please see Section 4.6 of our paper on overfitting issues). This can be achieved using R as follows: detail p.threshold - c ( 0.001 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ) # Read in the phenotype file phenotype - read.table ( EUR.height , header = T ) # Read in the PCs pcs - read.table ( EUR.eigenvec , header = F ) # The default output from plink does not include a header # To make things simple, we will add the appropriate headers # (1:6 because there are 6 PCs) colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) # Read in the covariates (here, it is sex) covariate - read.table ( EUR.covariate , header = T ) # Now merge the files pheno - merge ( merge ( phenotype , covariate , by = c ( FID , IID )), pcs , by = c ( FID , IID )) # We can then calculate the null model (model with PRS) using a linear regression # (as height is quantitative) null.model - lm ( Height ~ . , data = pheno [, ! colnames ( pheno ) %in% c ( FID , IID )]) # And the R2 of the null model is null.r2 - summary ( null.model ) $ r.squared prs.result - NULL for ( i in p.threshold ){ # Go through each p-value threshold prs - read.table ( paste0 ( EUR. , i , .profile ), header = T ) # Merge the prs with the phenotype matrix # We only want the FID, IID and PRS from the PRS file, therefore we only select the # relevant columns pheno.prs - merge ( pheno , prs [, c ( FID , IID , SCORE )], by = c ( FID , IID )) # Now perform a linear regression on Height with PRS and the covariates # ignoring the FID and IID from our model model - lm ( Height ~ . , data = pheno.prs [, ! colnames ( pheno.prs ) %in% c ( FID , IID )]) # model R2 is obtained as model.r2 - summary ( model ) $ r.squared # R2 of PRS is simply calculated as the model R2 minus the null R2 prs.r2 - model.r2 - null.r2 # We can also obtain the coeffcient and p-value of association of PRS as follow prs.coef - summary ( model ) $ coeff [ SCORE ,] prs.beta - as.numeric ( prs.coef [ 1 ]) prs.se - as.numeric ( prs.coef [ 2 ]) prs.p - as.numeric ( prs.coef [ 4 ]) # We can then store the results prs.result - rbind ( prs.result , data.frame ( Threshold = i , R2 = prs.r2 , P = prs.p , BETA = prs.beta , SE = prs.se )) } # Best result is: prs.result [ which.max ( prs.result $ R2 ),] q () # exit R quick p.threshold - c ( 0.001 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ) phenotype - read.table ( EUR.height , header = T ) pcs - read.table ( EUR.eigenvec , header = F ) colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) covariate - read.table ( EUR.covariate , header = T ) pheno - merge ( merge ( phenotype , covariate , by = c ( FID , IID )), pcs , by = c ( FID , IID )) null.r2 - summary ( lm ( Height ~ . , data = pheno [, ! colnames ( pheno ) %in% c ( FID , IID )])) $ r.squared prs.result - NULL for ( i in p.threshold ){ pheno.prs - merge ( pheno , read.table ( paste0 ( EUR. , i , .profile ), header = T )[, c ( FID , IID , SCORE )], by = c ( FID , IID )) model - summary ( lm ( Height ~ . , data = pheno.prs [, ! colnames ( pheno.prs ) %in% c ( FID , IID )])) model.r2 - model $ r.squared prs.r2 - model.r2 - null.r2 prs.coef - model $ coeff [ SCORE ,] prs.result - rbind ( prs.result , data.frame ( Threshold = i , R2 = prs.r2 , P = as.numeric ( prs.coef [ 4 ]), BETA = as.numeric ( prs.coef [ 1 ]), SE = as.numeric ( prs.coef [ 2 ]))) } print ( prs.result [ which.max ( prs.result $ R2 ),]) q () # exit R with data.table and magrittr library ( data.table ) library ( magrittr ) p.threshold - c ( 0.001 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ) phenotype - fread ( EUR.height ) pcs - fread ( EUR.eigenvec , header = F ) % % setnames ( . , colnames ( . ), c ( FID , IID , paste0 ( PC , 1 : 6 )) ) covariate - fread ( EUR.covariate ) pheno - merge ( phenotype , covariate ) % % merge ( . , pcs ) null.r2 - summary ( lm ( Height ~ . , data = pheno [, - c ( FID , IID )])) $ r.squared prs.result - NULL for ( i in p.threshold ){ pheno.prs - paste0 ( EUR. , i , .profile ) % % fread ( . ) % % . [, c ( FID , IID , SCORE )] % % merge ( . , pheno , by = c ( FID , IID )) model - lm ( Height ~ . , data = pheno.prs [, - c ( FID , IID )]) % % summary model.r2 - model $ r.squared prs.r2 - model.r2 - null.r2 prs.coef - model $ coeff [ SCORE ,] prs.result % % rbind ( . , data.frame ( Threshold = i , R2 = prs.r2 , P = as.numeric ( prs.coef [ 4 ]), BETA = as.numeric ( prs.coef [ 1 ]), SE = as.numeric ( prs.coef [ 2 ]))) } print ( prs.result [ which.max ( prs.result $ R2 ),]) q () # exit R Which P-value threshold generates the \"best-fit\" PRS? 0.1 How much phenotypic variation does the \"best-fit\" PRS explain? 0.04776492","title":"PLINK"},{"location":"plink/#background","text":"In this section of the tutorial you will use four different software programs to compute PRS from the base and target data that you QC'ed in the previous two sections. On this page, you will compute PRS using the popular genetic analyses tool plink - while plink is not a dedicated PRS software, you can perform every required steps of the C+T approach with plink . This multi-step process is a good way to learn the processes involved in computing PRS, which are typically performed automatically by PRS software.","title":"Background"},{"location":"plink/#required-data","text":"In the previous sections, we have generated the following files: File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.height This file contains the phenotype of the samples EUR.covariate This file contains the covariates of the samples","title":"Required Data"},{"location":"plink/#update-effect-size","text":"When the effect size relates to disease risk and is thus given as an odds ratio (OR), rather than BETA (for continuous traits), then the PRS is computed as a product of ORs. To simplify this calculation, we take the natural logarithm of the OR so that the PRS can be computed using summation instead (which can be back-transformed afterwards). We can obtain the transformed summary statistics with R : Without data.table dat - read.table ( gzfile ( Height.QC.gz ), header = T ) dat $ OR - log ( dat $ OR ) write.table ( dat , Height.QC.Transformed , quote = F , row.names = F ) q () # exit R With data.table library ( data.table ) dat - fread ( Height.QC.gz ) fwrite ( dat [, OR := log ( OR )], Height.QC.Transformed , sep = \\t ) q () # exit R Warning Due to rounding of values, using awk to log transform OR can lead to less accurate results. Therefore, we recommend performing the transformation in R or allow the PRS software to perform the transformation directly.","title":"Update Effect Size"},{"location":"plink/#clumping","text":"Linkage disequilibrium, which corresponds to the correlation between the genotypes of genetic variants across the genome, makes identifying the contribution from causal independent genetic variants extremely challenging. One way of approximately capturing the right level of causal signal is to perform clumping, which removes SNPs in ways that only weakly correlated SNPs are retained but preferentially retaining the SNPs most associated with the phenotype under study. Clumping can be performed using the following command in plink : plink \\ --bfile EUR.QC \\ --clump-p1 1 \\ --clump-r2 0 .1 \\ --clump-kb 250 \\ --clump Height.QC.Transformed \\ --clump-snp-field SNP \\ --clump-field P \\ --out EUR Each of the new parameters corresponds to the following Paramter Value Description clump-p1 1 P-value threshold for a SNP to be included as an index SNP. 1 is selected such that all SNPs are include for clumping clump-r2 0.1 SNPs having \\(r^2\\) higher than 0.1 with the index SNPs will be removed clump-kb 250 SNPs within 250k of the index SNP are considered for clumping clump Height.QC.Transformed Base data (summary statistic) file containing the P-value information clump-snp-field SNP Specifies that the column SNP contains the SNP IDs clump-field P Specifies that the column P contains the P-value information A more detailed description of the clumping process can be found here Note The \\(r^2\\) values computed by --clump are based on maximum likelihood haplotype frequency estimates This will generate EUR.clumped , containing the index SNPs after clumping is performed. We can extract the index SNP ID by performing the following command: awk NR!=1{print $3} EUR.clumped EUR.valid.snp $3 because the third column contains the SNP ID Note If your target data are small (e.g. N 500) then you can use the 1000 Genomes Project samples for the LD calculation. Make sure to use the population that most closely reflects represents the base sample.","title":"Clumping"},{"location":"plink/#generate-prs","text":"plink provides a convenient function --score and --q-score-range for calculating polygenic scores. We will need three files: The base data file: Height.QC.Transformed A file containing SNP IDs and their corresponding P-values ( $1 because SNP ID is located in the first column; $8 because the P-value is located in the eighth column) awk {print $1,$8} Height.QC.Transformed SNP.pvalue A file containing the different P-value thresholds for inclusion of SNPs in the PRS. Here calculate PRS corresponding to a few thresholds for illustration purposes: echo 0.001 0 0.001 range_list echo 0.05 0 0.05 range_list echo 0.1 0 0.1 range_list echo 0.2 0 0.2 range_list echo 0.3 0 0.3 range_list echo 0.4 0 0.4 range_list echo 0.5 0 0.5 range_list The format of the range_list file should be as follows: Name of Threshold Lower bound Upper Bound Note The threshold boundaries are inclusive. For example, for the 0.05 threshold, we include all SNPs with P-value from 0 to 0.05 , including any SNPs with P-value equal to 0.05 . We can then calculate the PRS with the following plink command: plink \\ --bfile EUR.QC \\ --score Height.QC.Transformed 1 4 11 header \\ --q-score-range range_list SNP.pvalue \\ --extract EUR.valid.snp \\ --out EUR The meaning of the new parameters are as follows: Paramter Value Description score Height.QC.Transformed 1 4 11 header We read from the Height.QC.Transformed file, assuming that the 1 st column is the SNP ID; 4 th column is the effective allele information; the 11 th column is the effect size estimate; and that the file contains a header q-score-range range_test SNP.pvalue We want to calculate PRS based on the thresholds defined in range_test , where the threshold values (P-values) were stored in SNP.pvalue The above command and range_list will generate 7 files: EUR.0.5.profile EUR.0.4.profile EUR.0.3.profile EUR.0.2.profile EUR.0.1.profile EUR.0.05.profile EUR.0.001.profile Note The default formula for PRS calculation in PLINK is: \\[ PRS_j =\\frac{ \\sum_i^NS_i*G_{ij}}{P*M_j} \\] where the effect size of SNP \\(i\\) is \\(S_i\\) ; the number of effect alleles observed in sample \\(j\\) is \\(G_{ij}\\) ; the ploidy of the sample is \\(P\\) (is generally 2 for humans); the total number of SNPs included in the PRS is \\(N\\) ; and the number of non-missing SNPs observed in sample \\(j\\) is \\(M_j\\) . If the sample has a missing genotype for SNP \\(i\\) , then the population minor allele frequency multiplied by the ploidy ( \\(MAF_i*P\\) ) is used instead of \\(G_{ij}\\) .","title":"Generate PRS"},{"location":"plink/#accounting-for-population-stratification","text":"Population structure is the principal source of confounding in GWAS and is usually accounted for by incorporating principal components (PCs) as covariates. We can incorporate PCs into our PRS analysis to account for population stratification. Again, we can calculate the PCs using plink : # First, we need to perform prunning plink \\ --bfile EUR.QC \\ --indep-pairwise 200 50 0 .25 \\ --out EUR # Then we calculate the first 6 PCs plink \\ --bfile EUR.QC \\ --extract EUR.prune.in \\ --pca 6 \\ --out EUR Note One way to select the appropriate number of PCs is to perform GWAS on the phenotype under study with different numbers of PCs. LDSC analysis can then be performed on the set of GWAS summary statistics and the GWAS that used the number of PCs that gave an LDSC intercept closest to 1 should correspond to that for which population structure was most accurately controlled for. Here the PCs have been stored in the EUR.eigenvec file and can be used as covariates in the regression model to account for population stratification. Important If the base and target samples are collected from different worldwide populations then the results from the PRS analysis may be biased (see Section 3.4 of our papper).","title":"Accounting for Population Stratification"},{"location":"plink/#finding-the-best-fit-prs","text":"The P-value threshold that provides the \"best-fit\" PRS under the C+T method is usually unknown. To approximate the \"best-fit\" PRS, we can perform a regression between PRS calculated at a range of P-value thresholds and then select the PRS that explains the highest phenotypic variance (please see Section 4.6 of our paper on overfitting issues). This can be achieved using R as follows: detail p.threshold - c ( 0.001 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ) # Read in the phenotype file phenotype - read.table ( EUR.height , header = T ) # Read in the PCs pcs - read.table ( EUR.eigenvec , header = F ) # The default output from plink does not include a header # To make things simple, we will add the appropriate headers # (1:6 because there are 6 PCs) colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) # Read in the covariates (here, it is sex) covariate - read.table ( EUR.covariate , header = T ) # Now merge the files pheno - merge ( merge ( phenotype , covariate , by = c ( FID , IID )), pcs , by = c ( FID , IID )) # We can then calculate the null model (model with PRS) using a linear regression # (as height is quantitative) null.model - lm ( Height ~ . , data = pheno [, ! colnames ( pheno ) %in% c ( FID , IID )]) # And the R2 of the null model is null.r2 - summary ( null.model ) $ r.squared prs.result - NULL for ( i in p.threshold ){ # Go through each p-value threshold prs - read.table ( paste0 ( EUR. , i , .profile ), header = T ) # Merge the prs with the phenotype matrix # We only want the FID, IID and PRS from the PRS file, therefore we only select the # relevant columns pheno.prs - merge ( pheno , prs [, c ( FID , IID , SCORE )], by = c ( FID , IID )) # Now perform a linear regression on Height with PRS and the covariates # ignoring the FID and IID from our model model - lm ( Height ~ . , data = pheno.prs [, ! colnames ( pheno.prs ) %in% c ( FID , IID )]) # model R2 is obtained as model.r2 - summary ( model ) $ r.squared # R2 of PRS is simply calculated as the model R2 minus the null R2 prs.r2 - model.r2 - null.r2 # We can also obtain the coeffcient and p-value of association of PRS as follow prs.coef - summary ( model ) $ coeff [ SCORE ,] prs.beta - as.numeric ( prs.coef [ 1 ]) prs.se - as.numeric ( prs.coef [ 2 ]) prs.p - as.numeric ( prs.coef [ 4 ]) # We can then store the results prs.result - rbind ( prs.result , data.frame ( Threshold = i , R2 = prs.r2 , P = prs.p , BETA = prs.beta , SE = prs.se )) } # Best result is: prs.result [ which.max ( prs.result $ R2 ),] q () # exit R quick p.threshold - c ( 0.001 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ) phenotype - read.table ( EUR.height , header = T ) pcs - read.table ( EUR.eigenvec , header = F ) colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) covariate - read.table ( EUR.covariate , header = T ) pheno - merge ( merge ( phenotype , covariate , by = c ( FID , IID )), pcs , by = c ( FID , IID )) null.r2 - summary ( lm ( Height ~ . , data = pheno [, ! colnames ( pheno ) %in% c ( FID , IID )])) $ r.squared prs.result - NULL for ( i in p.threshold ){ pheno.prs - merge ( pheno , read.table ( paste0 ( EUR. , i , .profile ), header = T )[, c ( FID , IID , SCORE )], by = c ( FID , IID )) model - summary ( lm ( Height ~ . , data = pheno.prs [, ! colnames ( pheno.prs ) %in% c ( FID , IID )])) model.r2 - model $ r.squared prs.r2 - model.r2 - null.r2 prs.coef - model $ coeff [ SCORE ,] prs.result - rbind ( prs.result , data.frame ( Threshold = i , R2 = prs.r2 , P = as.numeric ( prs.coef [ 4 ]), BETA = as.numeric ( prs.coef [ 1 ]), SE = as.numeric ( prs.coef [ 2 ]))) } print ( prs.result [ which.max ( prs.result $ R2 ),]) q () # exit R with data.table and magrittr library ( data.table ) library ( magrittr ) p.threshold - c ( 0.001 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ) phenotype - fread ( EUR.height ) pcs - fread ( EUR.eigenvec , header = F ) % % setnames ( . , colnames ( . ), c ( FID , IID , paste0 ( PC , 1 : 6 )) ) covariate - fread ( EUR.covariate ) pheno - merge ( phenotype , covariate ) % % merge ( . , pcs ) null.r2 - summary ( lm ( Height ~ . , data = pheno [, - c ( FID , IID )])) $ r.squared prs.result - NULL for ( i in p.threshold ){ pheno.prs - paste0 ( EUR. , i , .profile ) % % fread ( . ) % % . [, c ( FID , IID , SCORE )] % % merge ( . , pheno , by = c ( FID , IID )) model - lm ( Height ~ . , data = pheno.prs [, - c ( FID , IID )]) % % summary model.r2 - model $ r.squared prs.r2 - model.r2 - null.r2 prs.coef - model $ coeff [ SCORE ,] prs.result % % rbind ( . , data.frame ( Threshold = i , R2 = prs.r2 , P = as.numeric ( prs.coef [ 4 ]), BETA = as.numeric ( prs.coef [ 1 ]), SE = as.numeric ( prs.coef [ 2 ]))) } print ( prs.result [ which.max ( prs.result $ R2 ),]) q () # exit R Which P-value threshold generates the \"best-fit\" PRS? 0.1 How much phenotypic variation does the \"best-fit\" PRS explain? 0.04776492","title":"Finding the \"best-fit\" PRS"},{"location":"plink_visual/","text":"Plotting the Results The PRS results corresponding to a range of P-value thresholds obtained by application of the C+T PRS method (eg. using PLINK or PRSice-2) can be visualised using R as follows: Note We will be using prs.result , which was generated in the previous section Without ggplot2 # We strongly recommend the use of ggplot2. Only follow this code if you # are desperate. # Specify that we want to generate plot in EUR.height.bar.png png ( EUR.height.bar.png , height = 10 , width = 10 , res = 300 , unit = in ) # First, obtain the colorings based on the p-value col - suppressWarnings ( colorRampPalette ( c ( dodgerblue , firebrick ))) # We want the color gradient to match the ranking of p-values prs.result - prs.result [ order ( - log10 ( prs.result $ P )),] prs.result $ color - col ( nrow ( prs.result )) prs.result - prs.result [ order ( prs.result $ Threshold ),] # generate a pretty format for p-value output prs.result $ print.p - round ( prs.result $ P , digits = 3 ) prs.result $ print.p [ ! is.na ( prs.result $ print.p ) prs.result $ print.p == 0 ] - format ( prs.result $ P [ ! is.na ( prs.result $ print.p ) prs.result $ print.p == 0 ], digits = 2 ) prs.result $ print.p - sub ( e , *x*10^ , prs.result $ print.p ) # Generate the axis labels xlab - expression ( italic ( P ) - value ~ threshold ~ ( italic ( P )[ T ])) ylab - expression ( paste ( PRS model fit: , R ^ 2 )) # Setup the drawing area layout ( t ( 1 : 2 ), widths = c ( 8.8 , 1.2 )) par ( cex.lab = 1.5 , cex.axis = 1.25 , font.lab = 2 , oma = c ( 0 , 0.5 , 0 , 0 ), mar = c ( 4 , 6 , 0.5 , 0.5 )) # Plotting the bars b - barplot ( height = prs.result $ R2 , col = prs.result $ color , border = NA , ylim = c ( 0 , max ( prs.result $ R2 ) * 1.25 ), axes = F , ann = F ) # Plot the axis labels and axis ticks odd - seq ( 0 , nrow ( prs.result ) +1 , 2 ) even - seq ( 1 , nrow ( prs.result ), 2 ) axis ( side = 1 , at = b [ odd ], labels = prs.result $ Threshold [ odd ], lwd = 2 ) axis ( side = 1 , at = b [ even ], labels = prs.result $ Threshold [ even ], lwd = 2 ) axis ( side = 1 , at = c ( 0 , b [ 1 ], 2 * b [ length ( b )] - b [ length ( b ) -1 ]), labels = c ( , , ), lwd = 2 , lwd.tick = 0 ) # Write the p-value on top of each bar text ( parse ( text = paste ( prs.result $ print.p )), x = b +0.1 , y = prs.result $ R2 + ( max ( prs.result $ R2 ) * 1.05 - max ( prs.result $ R2 )), srt = 45 ) # Now plot the axis lines box ( bty = L , lwd = 2 ) axis ( 2 , las = 2 , lwd = 2 ) # Plot the axis titles title ( ylab = ylab , line = 4 , cex.lab = 1.5 , font = 2 ) title ( xlab = xlab , line = 2.5 , cex.lab = 1.5 , font = 2 ) # Generate plot area for the legend par ( cex.lab = 1.5 , cex.axis = 1.25 , font.lab = 2 , mar = c ( 20 , 0 , 20 , 4 )) prs.result - prs.result [ order ( - log10 ( prs.result $ P )),] image ( 1 , - log10 ( prs.result $ P ), t ( seq_along ( - log10 ( prs.result $ P ))), col = prs.result $ color , axes = F , ann = F ) axis ( 4 , las = 2 , xaxs = r , yaxs = r , tck = 0.2 , col = white ) # plot legend title title ( bquote ( atop ( - log [ 10 ] ~ model , italic ( P ) - value ), ), line = 2 , cex = 1.5 , font = 2 , adj = 0 ) # write the plot to file dev.off () q () # exit R ggplot2 # ggplot2 is a handy package for plotting library ( ggplot2 ) # generate a pretty format for p-value output prs.result $ print.p - round ( prs.result $ P , digits = 3 ) prs.result $ print.p [ ! is.na ( prs.result $ print.p ) prs.result $ print.p == 0 ] - format ( prs.result $ P [ ! is.na ( prs.result $ print.p ) prs.result $ print.p == 0 ], digits = 2 ) prs.result $ print.p - sub ( e , *x*10^ , prs.result $ print.p ) # Initialize ggplot, requiring the threshold as the x-axis (use factor so that it is uniformly distributed) ggplot ( data = prs.result , aes ( x = factor ( Threshold ), y = R2 )) + # Specify that we want to print p-value on top of the bars geom_text ( aes ( label = paste ( print.p )), vjust = -1.5 , hjust = 0 , angle = 45 , cex = 4 , parse = T ) + # Specify the range of the plot, *1.25 to provide enough space for the p-values scale_y_continuous ( limits = c ( 0 , max ( prs.result $ R2 ) * 1.25 )) + # Specify the axis labels xlab ( expression ( italic ( P ) - value ~ threshold ~ ( italic ( P )[ T ]))) + ylab ( expression ( paste ( PRS model fit: , R ^ 2 ))) + # Draw a bar plot geom_bar ( aes ( fill = - log10 ( P )), stat = identity ) + # Specify the colors scale_fill_gradient2 ( low = dodgerblue , high = firebrick , mid = dodgerblue , midpoint = 1e-4 , name = bquote ( atop ( - log [ 10 ] ~ model , italic ( P ) - value ),) ) + # Some beautification of the plot theme_classic () + theme ( axis.title = element_text ( face = bold , size = 18 ), axis.text = element_text ( size = 14 ), legend.title = element_text ( face = bold , size = 18 ), legend.text = element_text ( size = 14 ), axis.text.x = element_text ( angle = 45 , hjust = 1 ) ) # save the plot ggsave ( EUR.height.bar.png , height = 7 , width = 7 ) q () # exit R An example bar plot generated using ggplot2 In addition, we can visualise the relationship between the \"best-fit\" PRS (which may have been obtained from any of the PRS programs) and the phenotype of interest, coloured according to sex: Without ggplot2 # Read in the files prs - read.table ( EUR.0.2.profile , header = T ) height - read.table ( EUR.height , header = T ) sex - read.table ( EUR.covariate , header = T ) # Rename the sex sex $ Sex - as.factor ( sex $ Sex ) levels ( sex $ Sex ) - c ( Male , Female ) # Merge the files dat - merge ( merge ( prs , height ), sex ) # Start plotting plot ( x = dat $ SCORE , y = dat $ Height , col = white , xlab = Polygenic Score , ylab = Height ) with ( subset ( dat , Sex == Male ), points ( x = SCORE , y = Height , col = red )) with ( subset ( dat , Sex == Female ), points ( x = SCORE , y = Height , col = blue )) q () # exit R ggplot2 library ( ggplot2 ) # Read in the files prs - read.table ( EUR.0.2.profile , header = T ) height - read.table ( EUR.height , header = T ) sex - read.table ( EUR.covariate , header = T ) # Rename the sex sex $ Sex - as.factor ( sex $ Sex ) levels ( sex $ Sex ) - c ( Male , Female ) # Merge the files dat - merge ( merge ( prs , height ), sex ) # Start plotting ggplot ( dat , aes ( x = SCORE , y = Height , color = Sex )) + geom_point () + theme_classic () + labs ( x = Polygenic Score , y = Height ) q () # exit R An example scatter plot generated using ggplot2 Programs such as PRSice-2 and bigsnpr include numerous options for plotting PRS results.","title":"4. Visualizing PRS Results"},{"location":"plink_visual/#plotting-the-results","text":"The PRS results corresponding to a range of P-value thresholds obtained by application of the C+T PRS method (eg. using PLINK or PRSice-2) can be visualised using R as follows: Note We will be using prs.result , which was generated in the previous section Without ggplot2 # We strongly recommend the use of ggplot2. Only follow this code if you # are desperate. # Specify that we want to generate plot in EUR.height.bar.png png ( EUR.height.bar.png , height = 10 , width = 10 , res = 300 , unit = in ) # First, obtain the colorings based on the p-value col - suppressWarnings ( colorRampPalette ( c ( dodgerblue , firebrick ))) # We want the color gradient to match the ranking of p-values prs.result - prs.result [ order ( - log10 ( prs.result $ P )),] prs.result $ color - col ( nrow ( prs.result )) prs.result - prs.result [ order ( prs.result $ Threshold ),] # generate a pretty format for p-value output prs.result $ print.p - round ( prs.result $ P , digits = 3 ) prs.result $ print.p [ ! is.na ( prs.result $ print.p ) prs.result $ print.p == 0 ] - format ( prs.result $ P [ ! is.na ( prs.result $ print.p ) prs.result $ print.p == 0 ], digits = 2 ) prs.result $ print.p - sub ( e , *x*10^ , prs.result $ print.p ) # Generate the axis labels xlab - expression ( italic ( P ) - value ~ threshold ~ ( italic ( P )[ T ])) ylab - expression ( paste ( PRS model fit: , R ^ 2 )) # Setup the drawing area layout ( t ( 1 : 2 ), widths = c ( 8.8 , 1.2 )) par ( cex.lab = 1.5 , cex.axis = 1.25 , font.lab = 2 , oma = c ( 0 , 0.5 , 0 , 0 ), mar = c ( 4 , 6 , 0.5 , 0.5 )) # Plotting the bars b - barplot ( height = prs.result $ R2 , col = prs.result $ color , border = NA , ylim = c ( 0 , max ( prs.result $ R2 ) * 1.25 ), axes = F , ann = F ) # Plot the axis labels and axis ticks odd - seq ( 0 , nrow ( prs.result ) +1 , 2 ) even - seq ( 1 , nrow ( prs.result ), 2 ) axis ( side = 1 , at = b [ odd ], labels = prs.result $ Threshold [ odd ], lwd = 2 ) axis ( side = 1 , at = b [ even ], labels = prs.result $ Threshold [ even ], lwd = 2 ) axis ( side = 1 , at = c ( 0 , b [ 1 ], 2 * b [ length ( b )] - b [ length ( b ) -1 ]), labels = c ( , , ), lwd = 2 , lwd.tick = 0 ) # Write the p-value on top of each bar text ( parse ( text = paste ( prs.result $ print.p )), x = b +0.1 , y = prs.result $ R2 + ( max ( prs.result $ R2 ) * 1.05 - max ( prs.result $ R2 )), srt = 45 ) # Now plot the axis lines box ( bty = L , lwd = 2 ) axis ( 2 , las = 2 , lwd = 2 ) # Plot the axis titles title ( ylab = ylab , line = 4 , cex.lab = 1.5 , font = 2 ) title ( xlab = xlab , line = 2.5 , cex.lab = 1.5 , font = 2 ) # Generate plot area for the legend par ( cex.lab = 1.5 , cex.axis = 1.25 , font.lab = 2 , mar = c ( 20 , 0 , 20 , 4 )) prs.result - prs.result [ order ( - log10 ( prs.result $ P )),] image ( 1 , - log10 ( prs.result $ P ), t ( seq_along ( - log10 ( prs.result $ P ))), col = prs.result $ color , axes = F , ann = F ) axis ( 4 , las = 2 , xaxs = r , yaxs = r , tck = 0.2 , col = white ) # plot legend title title ( bquote ( atop ( - log [ 10 ] ~ model , italic ( P ) - value ), ), line = 2 , cex = 1.5 , font = 2 , adj = 0 ) # write the plot to file dev.off () q () # exit R ggplot2 # ggplot2 is a handy package for plotting library ( ggplot2 ) # generate a pretty format for p-value output prs.result $ print.p - round ( prs.result $ P , digits = 3 ) prs.result $ print.p [ ! is.na ( prs.result $ print.p ) prs.result $ print.p == 0 ] - format ( prs.result $ P [ ! is.na ( prs.result $ print.p ) prs.result $ print.p == 0 ], digits = 2 ) prs.result $ print.p - sub ( e , *x*10^ , prs.result $ print.p ) # Initialize ggplot, requiring the threshold as the x-axis (use factor so that it is uniformly distributed) ggplot ( data = prs.result , aes ( x = factor ( Threshold ), y = R2 )) + # Specify that we want to print p-value on top of the bars geom_text ( aes ( label = paste ( print.p )), vjust = -1.5 , hjust = 0 , angle = 45 , cex = 4 , parse = T ) + # Specify the range of the plot, *1.25 to provide enough space for the p-values scale_y_continuous ( limits = c ( 0 , max ( prs.result $ R2 ) * 1.25 )) + # Specify the axis labels xlab ( expression ( italic ( P ) - value ~ threshold ~ ( italic ( P )[ T ]))) + ylab ( expression ( paste ( PRS model fit: , R ^ 2 ))) + # Draw a bar plot geom_bar ( aes ( fill = - log10 ( P )), stat = identity ) + # Specify the colors scale_fill_gradient2 ( low = dodgerblue , high = firebrick , mid = dodgerblue , midpoint = 1e-4 , name = bquote ( atop ( - log [ 10 ] ~ model , italic ( P ) - value ),) ) + # Some beautification of the plot theme_classic () + theme ( axis.title = element_text ( face = bold , size = 18 ), axis.text = element_text ( size = 14 ), legend.title = element_text ( face = bold , size = 18 ), legend.text = element_text ( size = 14 ), axis.text.x = element_text ( angle = 45 , hjust = 1 ) ) # save the plot ggsave ( EUR.height.bar.png , height = 7 , width = 7 ) q () # exit R An example bar plot generated using ggplot2 In addition, we can visualise the relationship between the \"best-fit\" PRS (which may have been obtained from any of the PRS programs) and the phenotype of interest, coloured according to sex: Without ggplot2 # Read in the files prs - read.table ( EUR.0.2.profile , header = T ) height - read.table ( EUR.height , header = T ) sex - read.table ( EUR.covariate , header = T ) # Rename the sex sex $ Sex - as.factor ( sex $ Sex ) levels ( sex $ Sex ) - c ( Male , Female ) # Merge the files dat - merge ( merge ( prs , height ), sex ) # Start plotting plot ( x = dat $ SCORE , y = dat $ Height , col = white , xlab = Polygenic Score , ylab = Height ) with ( subset ( dat , Sex == Male ), points ( x = SCORE , y = Height , col = red )) with ( subset ( dat , Sex == Female ), points ( x = SCORE , y = Height , col = blue )) q () # exit R ggplot2 library ( ggplot2 ) # Read in the files prs - read.table ( EUR.0.2.profile , header = T ) height - read.table ( EUR.height , header = T ) sex - read.table ( EUR.covariate , header = T ) # Rename the sex sex $ Sex - as.factor ( sex $ Sex ) levels ( sex $ Sex ) - c ( Male , Female ) # Merge the files dat - merge ( merge ( prs , height ), sex ) # Start plotting ggplot ( dat , aes ( x = SCORE , y = Height , color = Sex )) + geom_point () + theme_classic () + labs ( x = Polygenic Score , y = Height ) q () # exit R An example scatter plot generated using ggplot2 Programs such as PRSice-2 and bigsnpr include numerous options for plotting PRS results.","title":"Plotting the Results"},{"location":"prsice/","text":"Over the following three pages you can run three dedicated PRS programs, which automate many of the steps from the previous page that used a sequence of PLINK functions (plus some QC steps). On this page you will run a PRS analysis using PRSice-2, which implements the standard C+T method. This analysis assumes that you have the following files (or you can download it from here ): File Name Description Height.QC.gz The post QC base data file. While PRSice-2 can automatically apply most filtering on the base file, it cannot remove duplicated SNPs EUR.QC.bed This file contains the genotype data that passed the QC steps EUR.QC.bim This file contains the list of SNPs that passed the QC steps EUR.QC.fam This file contains the samples that passed the QC steps EUR.height This file contains the phenotype data of the samples EUR.covariate This file contains the covariates of the samples EUR.eigenvec This file contains the principal components (PCs) of the samples And PRSice-2 , which can be downloaded from: Operating System Link Linux 64-bit v2.3.2 OS X 64-bit v2.3.2 In this tutorial, you will only need PRSice.R and PRSice_XXX where XXX is the operation system Running PRS analysis To run PRSice-2 we need a single covariate file, and therefore our covariate file and PCs file should be combined. This can be done with R as follows: without data.table covariate - read.table ( EUR.covariate , header = T ) pcs - read.table ( EUR.eigenvec , header = F ) colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) cov - merge ( covariate , pcs , by = c ( FID , IID )) write.table ( cov , EUR.cov , quote = F , row.names = F ) with data.table library ( data.table ) covariate - fread ( EUR.covariate ) pcs - fread ( EUR.eigenvec , header = F ) colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) cov - merge ( covariate , pcs ) fwrite ( cov , EUR.cov , sep = \\t ) which generates EUR.cov . PRSice-2 can then be run to obtain the PRS results as follows: Linux Rscript PRSice.R \\ --prsice PRSice_linux \\ --base Height.QC.gz \\ --target EUR.QC \\ --binary-target F \\ --pheno EUR.height \\ --cov EUR.cov \\ --base-maf MAF:0.01 \\ --base-info INFO:0.8 \\ --stat OR \\ --or \\ --out EUR OS X Rscript PRSice.R \\ --prsice PRSice_mac \\ --base Height.QC.gz \\ --target EUR.QC \\ --binary-target F \\ --pheno EUR.height \\ --cov EUR.cov \\ --base-maf MAF:0.01 \\ --base-info INFO:0.8 \\ --stat OR \\ --or \\ --out EUR Windows Rscript PRSice.R ^ --prsice PRSice_win64.exe ^ --base Height.QC.gz ^ --target EUR.QC ^ --binary-target F ^ --pheno EUR.height ^ --cov EUR.cov ^ --base-maf MAF:0.05 ^ --base-info INFO:0.8 ^ --stat OR ^ --or ^ --out EUR This will automatically perform \"high-resolution scoring\" and generate the \"best-fit\" PRS (in EUR.best ), with associated plots of the results. Users should read Section 4.6 of our paper to learn more about issues relating to overfitting in PRS analyses. Which P-value threshold generates the \"best-fit\" PRS? 0.15 How much phenotypic variation does the \"best-fit\" PRS explain? 0.0495036","title":"PRSice-2"},{"location":"prsice/#running-prs-analysis","text":"To run PRSice-2 we need a single covariate file, and therefore our covariate file and PCs file should be combined. This can be done with R as follows: without data.table covariate - read.table ( EUR.covariate , header = T ) pcs - read.table ( EUR.eigenvec , header = F ) colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) cov - merge ( covariate , pcs , by = c ( FID , IID )) write.table ( cov , EUR.cov , quote = F , row.names = F ) with data.table library ( data.table ) covariate - fread ( EUR.covariate ) pcs - fread ( EUR.eigenvec , header = F ) colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) cov - merge ( covariate , pcs ) fwrite ( cov , EUR.cov , sep = \\t ) which generates EUR.cov . PRSice-2 can then be run to obtain the PRS results as follows: Linux Rscript PRSice.R \\ --prsice PRSice_linux \\ --base Height.QC.gz \\ --target EUR.QC \\ --binary-target F \\ --pheno EUR.height \\ --cov EUR.cov \\ --base-maf MAF:0.01 \\ --base-info INFO:0.8 \\ --stat OR \\ --or \\ --out EUR OS X Rscript PRSice.R \\ --prsice PRSice_mac \\ --base Height.QC.gz \\ --target EUR.QC \\ --binary-target F \\ --pheno EUR.height \\ --cov EUR.cov \\ --base-maf MAF:0.01 \\ --base-info INFO:0.8 \\ --stat OR \\ --or \\ --out EUR Windows Rscript PRSice.R ^ --prsice PRSice_win64.exe ^ --base Height.QC.gz ^ --target EUR.QC ^ --binary-target F ^ --pheno EUR.height ^ --cov EUR.cov ^ --base-maf MAF:0.05 ^ --base-info INFO:0.8 ^ --stat OR ^ --or ^ --out EUR This will automatically perform \"high-resolution scoring\" and generate the \"best-fit\" PRS (in EUR.best ), with associated plots of the results. Users should read Section 4.6 of our paper to learn more about issues relating to overfitting in PRS analyses. Which P-value threshold generates the \"best-fit\" PRS? 0.15 How much phenotypic variation does the \"best-fit\" PRS explain? 0.0495036","title":"Running PRS analysis"},{"location":"target/","text":"Obtaining the target data Target data consist of individual-level genotype-phenotype data, usually generated within your lab/department/collaboration. For this tutorial, we have simulated some genotype-phenotype data using the 1000 Genomes Project European samples. You can download the data here Unzip the data as follow: unzip EUR.zip Note You will need plink in this section, which can be download from here . Install the program plink and include its location in your PATH directory, which allows us to use plink instead of ./plink in the commands below. If PLINK is not in your PATH directory and is instead in your working directory, replace all instances of plink in the tutorial with ./plink . QC checklist: Target data Below are the QC steps that comprise the QC checklist for the target data. # Sample size We recommend that users only perform PRS analyses on target data of at least 100 individuals. The sample size of our target data here is 503 individuals. # File transfer Usually we do not need to download and transfer the target data file because it is typically generated locally. However, the file should contain an md5sum code in case we send the data file to collaborators who may want to confirm that the file has not changed during the transfer. What is the md5sum code for each of the target files? File md5sum EUR.bed 940f5a760b41270662eba6264b262a2d EUR.bim a528020cc2448aa04a7499f13bf9f16a EUR.covariate afff13f8f9e15815f2237a62b8bec00b EUR.fam 17e8184fb03c690db6980bb7499d4982 EUR.height de68203f7f35744e987981ec5ffad35e # Genome build As stated in the base data section, the genome build for our base and target data is the same, as it should be. # Standard GWAS QC The target data must be quality controlled to at least the standards implemented in GWAS studies, e.g. removing SNPs with low genotyping rate, low minor allele frequency, out of Hardy-Weinberg Equilibrium, removing individuals with low genotyping rate (see Marees et al ). The following plink command applies some of these QC metrics to the target data: plink \\ --bfile EUR \\ --maf 0 .05 \\ --hwe 1e-6 \\ --geno 0 .01 \\ --mind 0 .01 \\ --write-snplist \\ --make-just-fam \\ --out EUR.QC Each of the parameters corresponds to the following Paramter Value Description bfile EUR Informs plink that the input genotype files should have a prefix of EUR maf 0.05 Removes all SNPs with minor allele frequency less than 0.05. Genotyping errors typically have a larger influence on SNPs with low MAF. Studies with large sample sizes could apply a lower MAF threshold hwe 1e-6 Removes SNPs with low P-value from the Hardy-Weinberg Equilibrium Fisher's exact or chi-squared test. SNPs with significant P-values from the HWE test are more likely affected by genotyping error or the effects of natural selection. Filtering should be performed on the control samples to avoid filtering SNPs that are causal (under selection in cases) geno 0.01 Excludes SNPs that are missing in a high fraction of subjects. A two-stage filtering process is usually performed (see Marees et al ). mind 0.01 Excludes individuals who have a high rate of genotype missingness, since this may indicate problems in the DNA sample or processing. (see Marees et al for more details). make-just-fam - Informs plink to only generate the QC'ed sample name to avoid generating the .bed file. write-snplist - Informs plink to only generate the QC'ed SNP list to avoid generating the .bed file. out EUR.QC Informs plink that all output should have a prefix of EUR.QC How many SNPs and samples were filtered? 5 samples were removed due to a high rate of genotype missingness 1 SNP were removed due to missing genotype data 872 SNPs were removed due to being out of Hardy-Weinberg Equilibrium 242,459 SNPs were removed due to low minor allele frequency Note Normally, we can generate a new genotype file using the new sample list. However, this will use up a lot of storage space. Using plink 's --extract , --exclude , --keep , --remove , --make-just-fam and --write-snplist functions, we can work solely on the list of samples and SNPs without duplicating the genotype file, reducing the storage space usage. Very high or low heterozygosity rates in individuals could be due to DNA contamination or to high levels of inbreeding. Therefore, samples with extreme heterozygosity are typically removed prior to downstream analyses. First, we perform prunning to remove highly correlated SNPs: plink \\ --bfile EUR \\ --keep EUR.QC.fam \\ --extract EUR.QC.snplist \\ --indep-pairwise 200 50 0 .25 \\ --out EUR.QC Each of the parameters corresponds to the following Paramter Value Description bfile EUR Informs plink that the input genotype files should have a prefix of EUR keep EUR.QC.fam Informs plink that we only want to use samples in EUR.QC.fam in the analysis extract EUR.QC.snplist Informs plink that we only want to use SNPs in EUR.QC.snplist in the analysis indep-pairwise 200 50 0.25 Informs plink that we wish to perform prunning with a window size of 200kb, sliding across the genome with step size of 50 variants at a time, and filter out any SNPs with LD \\(r^2\\) higher than 0.25 out EUR.QC Informs plink that all output should have a prefix of EUR.QC This will generate two files 1) EUR.QC.prune.in and 2) EUR.QC.prune.out . All SNPs within EUR.QC.prune.in have a pairwise \\(r^2 0.25\\) . Heterozygosity rates can then be computed using plink : plink \\ --bfile EUR \\ --extract EUR.QC.prune.in \\ --keep EUR.QC.fam \\ --het \\ --out EUR.QC This will generate the EUR.QC.het file, which contains F coefficient estimates for assessing heterozygosity. We will remove individuals with F coefficients that are more than 3 standard deviation (SD) units from the mean, which can be performed using the following R command (assuming that you have R downloaded, then you can open an R session by typing R in your terminal): Without library dat - read.table ( EUR.QC.het , header = T ) # Read in the EUR.het file, specify it has header m - mean ( dat $ F ) # Calculate the mean s - sd ( dat $ F ) # Calculate the SD valid - subset ( dat , F = m +3 * s F = m -3 * s ) # Get any samples with F coefficient within 3 SD of the population mean write.table ( valid [, c ( 1 , 2 )], EUR.valid.sample , quote = F , row.names = F ) # print FID and IID for valid samples q () # exit R With data.table library ( data.table ) # Read in file dat - fread ( EUR.QC.het ) # Get samples with F coefficient within 3 SD of the population mean valid - dat [ F = mean ( F ) +3 * sd ( F ) F = mean ( F ) -3 * sd ( F )] # print FID and IID for valid samples fwrite ( valid [, c ( FID , IID )], EUR.valid.sample , sep = \\t ) q () # exit R How many samples were excluded due to high heterozygosity rate? 7 samples were excluded # Ambiguous SNPs These were removed during the base data QC. # Mismatching SNPs SNPs that have mismatching alleles reported in the base and target data may be resolvable by strand-flipping the alleles to their complementary alleles in e.g. the target data, such as for a SNP with A/C in the base data and G/T in the target. This can be achieved with the following steps: 1. Load the bim file, the GIANT summary statistic and the QC SNP list into R Without data.table # Read in bim file bim - read.table ( EUR.bim ) colnames ( bim ) - c ( CHR , SNP , CM , BP , B.A1 , B.A2 ) # Read in QCed SNPs qc - read.table ( EUR.QC.snplist , header = F , stringsAsFactors = F ) # Read in GIANT data height - read.table ( gzfile ( Height.QC.gz ), header = T , stringsAsFactors = F , sep = \\t ) # Change all alleles to upper case for easy comparison height $ A1 - toupper ( height $ A1 ) height $ A2 - toupper ( height $ A2 ) bim $ B.A1 - toupper ( bim $ B.A1 ) bim $ B.A2 - toupper ( bim $ B.A2 ) With data.table and magrittr # magrittr allow us to do piping, which help to reduce the # amount of intermediate data types library ( data.table ) library ( magrittr ) # Read in bim file bim - fread ( EUR.bim ) % % # Note: . represents the output from previous step # The syntax here means, setnames of the data read from # the bim file, and replace the original column names by # the new names setnames ( . , colnames ( . ), c ( CHR , SNP , CM , BP , B.A1 , B.A2 )) % % # And immediately change the alleles to upper cases . [, c ( B.A1 , B.A2 ) := list ( toupper ( B.A1 ), toupper ( B.A2 ))] # Read in GIANT data (require data.table v1.12.0+) height - fread ( Height.QC.gz ) % % # And immediately change the alleles to upper cases . [, c ( A1 , A2 ) := list ( toupper ( A1 ), toupper ( A2 ))] # Read in QCed SNPs qc - fread ( EUR.QC.snplist , header = F ) 2. Identify SNPs that require strand flipping Without data.table # Merge GIANT with target info - merge ( bim , height , by = c ( SNP , CHR , BP )) # Filter QCed SNPs info - info [ info $ SNP %in% qc $ V1 ,] # Function for finding the complementary allele complement - function ( x ) { switch ( x , A = T , C = G , T = A , G = C , return ( NA ) ) } # Get SNPs that have the same alleles across base and target info.match - subset ( info , A1 == B.A1 A2 == B.A2 ) # Identify SNPs that are complementary between base and target info $ C.A1 - sapply ( info $ B.A1 , complement ) info $ C.A2 - sapply ( info $ B.A2 , complement ) info.complement - subset ( info , A1 == C.A1 A2 == C.A2 ) # Update the complementary alleles in the bim file # This allow us to match the allele in subsequent analysis complement.snps - bim $ SNP %in% info.complement $ SNP bim [ complement.snps ,] $ B.A1 - sapply ( bim [ complement.snps ,] $ B.A1 , complement ) bim [ complement.snps ,] $ B.A2 - sapply ( bim [ complement.snps ,] $ B.A2 , complement ) With data.table and magrittr # Merge GIANT with target info - merge ( bim , height , by = c ( SNP , CHR , BP )) % % # And filter out QCed SNPs . [ SNP %in% qc [, V1 ]] # Function for calculating the complementary allele complement - function ( x ){ switch ( x , A = T , C = G , T = A , G = C , return ( NA ) ) } # Get SNPs that have the same alleles across base and target info.match - info [ A1 == B.A1 A2 == B.A2 , SNP ] # Identify SNPs that are complementary between base and target com.snps - info [ sapply ( B.A1 , complement ) == A1 sapply ( B.A2 , complement ) == A2 , SNP ] # Now update the bim file bim [ SNP %in% com.snps , c ( B.A1 , B.A2 ) := list ( sapply ( B.A1 , complement ), sapply ( B.A2 , complement ))] 3. Identify SNPs that require recoding in the target (to ensure the coding allele in the target data is the effective allele in the base summary statistic) Without data.table # identify SNPs that need recoding info.recode - subset ( info , A1 == B.A2 A2 == B.A1 ) # Update the recode SNPs recode.snps - bim $ SNP %in% info.recode $ SNP tmp - bim [ recode.snps ,] $ B.A1 bim [ recode.snps ,] $ B.A1 - bim [ recode.snps ,] $ B.A2 bim [ recode.snps ,] $ B.A2 - tmp # identify SNPs that need recoding complement info.crecode - subset ( info , A1 == C.A2 A2 == C.A1 ) # Update the recode + strand flip SNPs com.snps - bim $ SNP %in% info.crecode $ SNP tmp - bim [ com.snps ,] $ B.A1 bim [ com.snps ,] $ B.A1 - as.character ( sapply ( bim [ com.snps ,] $ B.A2 , complement )) bim [ com.snps ,] $ B.A2 - as.character ( sapply ( tmp , complement )) # Output updated bim file write.table ( bim , EUR.QC.adj.bim , quote = F , row.names = F , col.names = F , sep = \\t ) With data.table and magrittr # identify SNPs that need recoding recode.snps - info [ B.A1 == A2 B.A2 == A1 , SNP ] # Update the bim file bim [ SNP %in% recode.snps , c ( B.A1 , B.A2 ) := list ( B.A2 , B.A1 )] # identify SNPs that need recoding complement com.recode - info [ sapply ( B.A1 , complement ) == A2 sapply ( B.A2 , complement ) == A1 , SNP ] # Now update the bim file bim [ SNP %in% com.recode , c ( B.A1 , B.A2 ) := list ( sapply ( B.A2 , complement ), sapply ( B.A1 , complement ))] # Write the updated bim file fwrite ( bim , EUR.QC.adj.bim , col.names = F , sep = \\t ) 4. Identify SNPs that have different allele in base and target (usually due to difference in genome build or Indel) Without data.table mismatch - bim $ SNP [ ! ( bim $ SNP %in% info.match $ SNP | bim $ SNP %in% info.complement $ SNP | bim $ SNP %in% info.recode $ SNP | bim $ SNP %in% info.crecode $ SNP )] write.table ( mismatch , EUR.mismatch , quote = F , row.names = F , col.names = F ) q () # exit R With data.table mismatch - bim [ ! ( SNP %in% info.match | SNP %in% com.snps | SNP %in% recode.snps | SNP %in% com.recode ), SNP ] write.table ( mismatch , EUR.mismatch , quote = F , row.names = F , col.names = F ) q () # exit R 5. Replace EUR.bim with EUR.QC.adj.bim : # Make a back up mv EUR.bim EUR.bim.bk ln -s EUR.QC.adj.bim EUR.bim The above commands do the following: Rename EUR.bim to EUR.bim.bk Soft linking ( ln -s ) EUR.QC.adj.bim as EUR.bim Note Most PRS software will perform strand-flipping automatically, thus this step is usually not required. # Duplicate SNPs Make sure to remove any duplicate SNPs in your target data (these target data were simulated and so include no duplicated SNPs). # Sex chromosomes Sometimes sample mislabelling can occur, which may lead to invalid results. One indication of a mislabelled sample is a difference between reported sex and that indicated by the sex chromosomes. While this may be due to a difference in sex and gender identity, it could also reflect mislabeling of samples or misreporting and, thus, individuals in which there is a mismatch between biological and reported sex are typically removed. A sex check can be performed in PLINK, in which individuals are called as females if their X chromosome homozygosity estimate (F statistic) is 0.2 and as males if the estimate is 0.8. Before performing a sex check, pruning should be performed (see here ). A sex check can then easily be conducted using plink plink \\ --bfile EUR \\ --extract EUR.QC.prune.in \\ --keep EUR.valid.sample \\ --check-sex \\ --out EUR.QC This will generate a file called EUR.QC.sexcheck containing the F-statistics for each individual. Individuals are typically called as being biologically male if the F-statistic is 0.8 and biologically female if F 0.2. Without library # Read in file valid - read.table ( EUR.valid.sample , header = T ) dat - read.table ( EUR.QC.sexcheck , header = T ) valid - subset ( dat , STATUS == OK FID %in% valid $ FID ) write.table ( valid [, c ( FID , IID )], EUR.QC.valid , row.names = F , col.names = F , sep = \\t , quote = F ) q () # exit R With data.table library ( data.table ) # Read in file valid - fread ( EUR.valid.sample ) dat - fread ( EUR.QC.sexcheck )[ FID %in% valid $ FID ] fwrite ( dat [ STATUS == OK , c ( FID , IID )], EUR.QC.valid , sep = \\t ) q () # exit R How many samples were excluded due mismatched Sex information? 2 samples were excluded # Sample overlap Since the target data were simulated there are no overlapping samples between the base and target data here (see the relevant section of the paper for discussion of the importance of avoiding sample overlap). # Relatedness Closely related individuals in the target data may lead to overfitted results, limiting the generalisability of the results. Before calculating the relatedness, pruning should be performed (see here ). Individuals that have a first or second degree relative in the sample ( \\(\\hat{\\pi} 0.125\\) ) can be removed with the following command: plink \\ --bfile EUR \\ --extract EUR.QC.prune.in \\ --keep EUR.QC.valid \\ --rel-cutoff 0 .125 \\ --out EUR.QC How many related samples were excluded? 2 samples were excluded Note A greedy algorithm is used to remove closely related individuals in a way that optimises the size of the sample retained. However, the algorithm is dependent on the random seed used, which can generate different results. Therefore, to reproduce the same result, you will need to specify the same random seed. PLINK's algorithm for removing related individuals does not account for the phenotype under study. To minimize the removal of cases of a disease, the following algorithm can be used instead: GreedyRelated . Generate final QC'ed target data file After performing the full analysis, you can generate a QC'ed data set with the following command: plink \\ --bfile EUR \\ --make-bed \\ --keep EUR.QC.rel.id \\ --out EUR.QC \\ --extract EUR.QC.snplist \\ --exclude EUR.mismatch","title":"2. QC of Target Data"},{"location":"target/#obtaining-the-target-data","text":"Target data consist of individual-level genotype-phenotype data, usually generated within your lab/department/collaboration. For this tutorial, we have simulated some genotype-phenotype data using the 1000 Genomes Project European samples. You can download the data here Unzip the data as follow: unzip EUR.zip Note You will need plink in this section, which can be download from here . Install the program plink and include its location in your PATH directory, which allows us to use plink instead of ./plink in the commands below. If PLINK is not in your PATH directory and is instead in your working directory, replace all instances of plink in the tutorial with ./plink .","title":"Obtaining the target data"},{"location":"target/#qc-checklist-target-data","text":"Below are the QC steps that comprise the QC checklist for the target data.","title":"QC checklist: Target data"},{"location":"target/#35-sample-size","text":"We recommend that users only perform PRS analyses on target data of at least 100 individuals. The sample size of our target data here is 503 individuals.","title":"# Sample size"},{"location":"target/#35-file-transfer","text":"Usually we do not need to download and transfer the target data file because it is typically generated locally. However, the file should contain an md5sum code in case we send the data file to collaborators who may want to confirm that the file has not changed during the transfer. What is the md5sum code for each of the target files? File md5sum EUR.bed 940f5a760b41270662eba6264b262a2d EUR.bim a528020cc2448aa04a7499f13bf9f16a EUR.covariate afff13f8f9e15815f2237a62b8bec00b EUR.fam 17e8184fb03c690db6980bb7499d4982 EUR.height de68203f7f35744e987981ec5ffad35e","title":"# File transfer"},{"location":"target/#35-genome-build","text":"As stated in the base data section, the genome build for our base and target data is the same, as it should be.","title":"# Genome build"},{"location":"target/#35-standard-gwas-qc","text":"The target data must be quality controlled to at least the standards implemented in GWAS studies, e.g. removing SNPs with low genotyping rate, low minor allele frequency, out of Hardy-Weinberg Equilibrium, removing individuals with low genotyping rate (see Marees et al ). The following plink command applies some of these QC metrics to the target data: plink \\ --bfile EUR \\ --maf 0 .05 \\ --hwe 1e-6 \\ --geno 0 .01 \\ --mind 0 .01 \\ --write-snplist \\ --make-just-fam \\ --out EUR.QC Each of the parameters corresponds to the following Paramter Value Description bfile EUR Informs plink that the input genotype files should have a prefix of EUR maf 0.05 Removes all SNPs with minor allele frequency less than 0.05. Genotyping errors typically have a larger influence on SNPs with low MAF. Studies with large sample sizes could apply a lower MAF threshold hwe 1e-6 Removes SNPs with low P-value from the Hardy-Weinberg Equilibrium Fisher's exact or chi-squared test. SNPs with significant P-values from the HWE test are more likely affected by genotyping error or the effects of natural selection. Filtering should be performed on the control samples to avoid filtering SNPs that are causal (under selection in cases) geno 0.01 Excludes SNPs that are missing in a high fraction of subjects. A two-stage filtering process is usually performed (see Marees et al ). mind 0.01 Excludes individuals who have a high rate of genotype missingness, since this may indicate problems in the DNA sample or processing. (see Marees et al for more details). make-just-fam - Informs plink to only generate the QC'ed sample name to avoid generating the .bed file. write-snplist - Informs plink to only generate the QC'ed SNP list to avoid generating the .bed file. out EUR.QC Informs plink that all output should have a prefix of EUR.QC How many SNPs and samples were filtered? 5 samples were removed due to a high rate of genotype missingness 1 SNP were removed due to missing genotype data 872 SNPs were removed due to being out of Hardy-Weinberg Equilibrium 242,459 SNPs were removed due to low minor allele frequency Note Normally, we can generate a new genotype file using the new sample list. However, this will use up a lot of storage space. Using plink 's --extract , --exclude , --keep , --remove , --make-just-fam and --write-snplist functions, we can work solely on the list of samples and SNPs without duplicating the genotype file, reducing the storage space usage. Very high or low heterozygosity rates in individuals could be due to DNA contamination or to high levels of inbreeding. Therefore, samples with extreme heterozygosity are typically removed prior to downstream analyses. First, we perform prunning to remove highly correlated SNPs: plink \\ --bfile EUR \\ --keep EUR.QC.fam \\ --extract EUR.QC.snplist \\ --indep-pairwise 200 50 0 .25 \\ --out EUR.QC Each of the parameters corresponds to the following Paramter Value Description bfile EUR Informs plink that the input genotype files should have a prefix of EUR keep EUR.QC.fam Informs plink that we only want to use samples in EUR.QC.fam in the analysis extract EUR.QC.snplist Informs plink that we only want to use SNPs in EUR.QC.snplist in the analysis indep-pairwise 200 50 0.25 Informs plink that we wish to perform prunning with a window size of 200kb, sliding across the genome with step size of 50 variants at a time, and filter out any SNPs with LD \\(r^2\\) higher than 0.25 out EUR.QC Informs plink that all output should have a prefix of EUR.QC This will generate two files 1) EUR.QC.prune.in and 2) EUR.QC.prune.out . All SNPs within EUR.QC.prune.in have a pairwise \\(r^2 0.25\\) . Heterozygosity rates can then be computed using plink : plink \\ --bfile EUR \\ --extract EUR.QC.prune.in \\ --keep EUR.QC.fam \\ --het \\ --out EUR.QC This will generate the EUR.QC.het file, which contains F coefficient estimates for assessing heterozygosity. We will remove individuals with F coefficients that are more than 3 standard deviation (SD) units from the mean, which can be performed using the following R command (assuming that you have R downloaded, then you can open an R session by typing R in your terminal): Without library dat - read.table ( EUR.QC.het , header = T ) # Read in the EUR.het file, specify it has header m - mean ( dat $ F ) # Calculate the mean s - sd ( dat $ F ) # Calculate the SD valid - subset ( dat , F = m +3 * s F = m -3 * s ) # Get any samples with F coefficient within 3 SD of the population mean write.table ( valid [, c ( 1 , 2 )], EUR.valid.sample , quote = F , row.names = F ) # print FID and IID for valid samples q () # exit R With data.table library ( data.table ) # Read in file dat - fread ( EUR.QC.het ) # Get samples with F coefficient within 3 SD of the population mean valid - dat [ F = mean ( F ) +3 * sd ( F ) F = mean ( F ) -3 * sd ( F )] # print FID and IID for valid samples fwrite ( valid [, c ( FID , IID )], EUR.valid.sample , sep = \\t ) q () # exit R How many samples were excluded due to high heterozygosity rate? 7 samples were excluded","title":"# Standard GWAS QC"},{"location":"target/#35-ambiguous-snps","text":"These were removed during the base data QC.","title":"# Ambiguous SNPs"},{"location":"target/#35-mismatching-snps","text":"SNPs that have mismatching alleles reported in the base and target data may be resolvable by strand-flipping the alleles to their complementary alleles in e.g. the target data, such as for a SNP with A/C in the base data and G/T in the target. This can be achieved with the following steps: 1. Load the bim file, the GIANT summary statistic and the QC SNP list into R Without data.table # Read in bim file bim - read.table ( EUR.bim ) colnames ( bim ) - c ( CHR , SNP , CM , BP , B.A1 , B.A2 ) # Read in QCed SNPs qc - read.table ( EUR.QC.snplist , header = F , stringsAsFactors = F ) # Read in GIANT data height - read.table ( gzfile ( Height.QC.gz ), header = T , stringsAsFactors = F , sep = \\t ) # Change all alleles to upper case for easy comparison height $ A1 - toupper ( height $ A1 ) height $ A2 - toupper ( height $ A2 ) bim $ B.A1 - toupper ( bim $ B.A1 ) bim $ B.A2 - toupper ( bim $ B.A2 ) With data.table and magrittr # magrittr allow us to do piping, which help to reduce the # amount of intermediate data types library ( data.table ) library ( magrittr ) # Read in bim file bim - fread ( EUR.bim ) % % # Note: . represents the output from previous step # The syntax here means, setnames of the data read from # the bim file, and replace the original column names by # the new names setnames ( . , colnames ( . ), c ( CHR , SNP , CM , BP , B.A1 , B.A2 )) % % # And immediately change the alleles to upper cases . [, c ( B.A1 , B.A2 ) := list ( toupper ( B.A1 ), toupper ( B.A2 ))] # Read in GIANT data (require data.table v1.12.0+) height - fread ( Height.QC.gz ) % % # And immediately change the alleles to upper cases . [, c ( A1 , A2 ) := list ( toupper ( A1 ), toupper ( A2 ))] # Read in QCed SNPs qc - fread ( EUR.QC.snplist , header = F ) 2. Identify SNPs that require strand flipping Without data.table # Merge GIANT with target info - merge ( bim , height , by = c ( SNP , CHR , BP )) # Filter QCed SNPs info - info [ info $ SNP %in% qc $ V1 ,] # Function for finding the complementary allele complement - function ( x ) { switch ( x , A = T , C = G , T = A , G = C , return ( NA ) ) } # Get SNPs that have the same alleles across base and target info.match - subset ( info , A1 == B.A1 A2 == B.A2 ) # Identify SNPs that are complementary between base and target info $ C.A1 - sapply ( info $ B.A1 , complement ) info $ C.A2 - sapply ( info $ B.A2 , complement ) info.complement - subset ( info , A1 == C.A1 A2 == C.A2 ) # Update the complementary alleles in the bim file # This allow us to match the allele in subsequent analysis complement.snps - bim $ SNP %in% info.complement $ SNP bim [ complement.snps ,] $ B.A1 - sapply ( bim [ complement.snps ,] $ B.A1 , complement ) bim [ complement.snps ,] $ B.A2 - sapply ( bim [ complement.snps ,] $ B.A2 , complement ) With data.table and magrittr # Merge GIANT with target info - merge ( bim , height , by = c ( SNP , CHR , BP )) % % # And filter out QCed SNPs . [ SNP %in% qc [, V1 ]] # Function for calculating the complementary allele complement - function ( x ){ switch ( x , A = T , C = G , T = A , G = C , return ( NA ) ) } # Get SNPs that have the same alleles across base and target info.match - info [ A1 == B.A1 A2 == B.A2 , SNP ] # Identify SNPs that are complementary between base and target com.snps - info [ sapply ( B.A1 , complement ) == A1 sapply ( B.A2 , complement ) == A2 , SNP ] # Now update the bim file bim [ SNP %in% com.snps , c ( B.A1 , B.A2 ) := list ( sapply ( B.A1 , complement ), sapply ( B.A2 , complement ))] 3. Identify SNPs that require recoding in the target (to ensure the coding allele in the target data is the effective allele in the base summary statistic) Without data.table # identify SNPs that need recoding info.recode - subset ( info , A1 == B.A2 A2 == B.A1 ) # Update the recode SNPs recode.snps - bim $ SNP %in% info.recode $ SNP tmp - bim [ recode.snps ,] $ B.A1 bim [ recode.snps ,] $ B.A1 - bim [ recode.snps ,] $ B.A2 bim [ recode.snps ,] $ B.A2 - tmp # identify SNPs that need recoding complement info.crecode - subset ( info , A1 == C.A2 A2 == C.A1 ) # Update the recode + strand flip SNPs com.snps - bim $ SNP %in% info.crecode $ SNP tmp - bim [ com.snps ,] $ B.A1 bim [ com.snps ,] $ B.A1 - as.character ( sapply ( bim [ com.snps ,] $ B.A2 , complement )) bim [ com.snps ,] $ B.A2 - as.character ( sapply ( tmp , complement )) # Output updated bim file write.table ( bim , EUR.QC.adj.bim , quote = F , row.names = F , col.names = F , sep = \\t ) With data.table and magrittr # identify SNPs that need recoding recode.snps - info [ B.A1 == A2 B.A2 == A1 , SNP ] # Update the bim file bim [ SNP %in% recode.snps , c ( B.A1 , B.A2 ) := list ( B.A2 , B.A1 )] # identify SNPs that need recoding complement com.recode - info [ sapply ( B.A1 , complement ) == A2 sapply ( B.A2 , complement ) == A1 , SNP ] # Now update the bim file bim [ SNP %in% com.recode , c ( B.A1 , B.A2 ) := list ( sapply ( B.A2 , complement ), sapply ( B.A1 , complement ))] # Write the updated bim file fwrite ( bim , EUR.QC.adj.bim , col.names = F , sep = \\t ) 4. Identify SNPs that have different allele in base and target (usually due to difference in genome build or Indel) Without data.table mismatch - bim $ SNP [ ! ( bim $ SNP %in% info.match $ SNP | bim $ SNP %in% info.complement $ SNP | bim $ SNP %in% info.recode $ SNP | bim $ SNP %in% info.crecode $ SNP )] write.table ( mismatch , EUR.mismatch , quote = F , row.names = F , col.names = F ) q () # exit R With data.table mismatch - bim [ ! ( SNP %in% info.match | SNP %in% com.snps | SNP %in% recode.snps | SNP %in% com.recode ), SNP ] write.table ( mismatch , EUR.mismatch , quote = F , row.names = F , col.names = F ) q () # exit R 5. Replace EUR.bim with EUR.QC.adj.bim : # Make a back up mv EUR.bim EUR.bim.bk ln -s EUR.QC.adj.bim EUR.bim The above commands do the following: Rename EUR.bim to EUR.bim.bk Soft linking ( ln -s ) EUR.QC.adj.bim as EUR.bim Note Most PRS software will perform strand-flipping automatically, thus this step is usually not required.","title":"# Mismatching SNPs"},{"location":"target/#35-duplicate-snps","text":"Make sure to remove any duplicate SNPs in your target data (these target data were simulated and so include no duplicated SNPs).","title":"# Duplicate SNPs"},{"location":"target/#35-sex-chromosomes","text":"Sometimes sample mislabelling can occur, which may lead to invalid results. One indication of a mislabelled sample is a difference between reported sex and that indicated by the sex chromosomes. While this may be due to a difference in sex and gender identity, it could also reflect mislabeling of samples or misreporting and, thus, individuals in which there is a mismatch between biological and reported sex are typically removed. A sex check can be performed in PLINK, in which individuals are called as females if their X chromosome homozygosity estimate (F statistic) is 0.2 and as males if the estimate is 0.8. Before performing a sex check, pruning should be performed (see here ). A sex check can then easily be conducted using plink plink \\ --bfile EUR \\ --extract EUR.QC.prune.in \\ --keep EUR.valid.sample \\ --check-sex \\ --out EUR.QC This will generate a file called EUR.QC.sexcheck containing the F-statistics for each individual. Individuals are typically called as being biologically male if the F-statistic is 0.8 and biologically female if F 0.2. Without library # Read in file valid - read.table ( EUR.valid.sample , header = T ) dat - read.table ( EUR.QC.sexcheck , header = T ) valid - subset ( dat , STATUS == OK FID %in% valid $ FID ) write.table ( valid [, c ( FID , IID )], EUR.QC.valid , row.names = F , col.names = F , sep = \\t , quote = F ) q () # exit R With data.table library ( data.table ) # Read in file valid - fread ( EUR.valid.sample ) dat - fread ( EUR.QC.sexcheck )[ FID %in% valid $ FID ] fwrite ( dat [ STATUS == OK , c ( FID , IID )], EUR.QC.valid , sep = \\t ) q () # exit R How many samples were excluded due mismatched Sex information? 2 samples were excluded","title":"# Sex chromosomes"},{"location":"target/#35-sample-overlap","text":"Since the target data were simulated there are no overlapping samples between the base and target data here (see the relevant section of the paper for discussion of the importance of avoiding sample overlap).","title":"# Sample overlap"},{"location":"target/#35-relatedness","text":"Closely related individuals in the target data may lead to overfitted results, limiting the generalisability of the results. Before calculating the relatedness, pruning should be performed (see here ). Individuals that have a first or second degree relative in the sample ( \\(\\hat{\\pi} 0.125\\) ) can be removed with the following command: plink \\ --bfile EUR \\ --extract EUR.QC.prune.in \\ --keep EUR.QC.valid \\ --rel-cutoff 0 .125 \\ --out EUR.QC How many related samples were excluded? 2 samples were excluded Note A greedy algorithm is used to remove closely related individuals in a way that optimises the size of the sample retained. However, the algorithm is dependent on the random seed used, which can generate different results. Therefore, to reproduce the same result, you will need to specify the same random seed. PLINK's algorithm for removing related individuals does not account for the phenotype under study. To minimize the removal of cases of a disease, the following algorithm can be used instead: GreedyRelated .","title":"# Relatedness"},{"location":"target/#generate-final-qced-target-data-file","text":"After performing the full analysis, you can generate a QC'ed data set with the following command: plink \\ --bfile EUR \\ --make-bed \\ --keep EUR.QC.rel.id \\ --out EUR.QC \\ --extract EUR.QC.snplist \\ --exclude EUR.mismatch","title":"Generate final QC'ed target data file"}]}